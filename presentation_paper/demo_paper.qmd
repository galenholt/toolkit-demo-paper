---
title: "An integrated toolkit for assessment of hydrology-dependent outcomes in the Murray-Darling Basin: HydroBOT"
author: 
 - name: Galen Holt
   orcid: 0000-0002-7455-9275
   corresponding: true
   email: galen@deakin.edu.au
   affiliation:
    - ref: du
 - name: Georgia Dwyer
   orcid: 
   corresponding: false
   affiliations:
     - ref: du
 - name: David Robertson
   orcid: 
   corresponding: false
   affiliations:
     - CSIRO
 - name: Martin Job
   orcid: 
   corresponding: false
   affiliations:
     - MDBA
 - name: Lara Palmer
   orcid: 
   corresponding: false
   affiliations:
     - MDBA
 - name: Rebecca E Lester
   orcid: 
   corresponding: false
   affiliations:
     - ref: du

affiliations:
  - id: du
    name: Deakin University
    city: Waurn Ponds
    state: Victoria
        
keywords: 
 - Murray-Darling Basin
 - Holistic modeling
 - Management modeling
 - Climate change
 - Climate adaptation
 
date: last-modified

bibliography: references.bib

number-sections: true

echo: false
---

```{r}
#| eval: false

# This should not be run, except to make a simple quarto
make_simpleyml <- function(renderfile = 'auto') {
  
  if (renderfile == 'auto') {
    projpath <- rstudioapi::getActiveProject()
    docpath <- rstudioapi::documentPath()
    projdir <- sub(".*/([^/]+)$", "\\1", projpath)
    reldocpath <- sub(paste0(".*", projdir, "/"), "", docpath)
    renderfile <- reldocpath
  }
    
  
  simple_yaml <- list()
  simple_yaml$project <- list()
  simple_yaml$project$render <- list(renderfile)
  yaml::write_yaml(simple_yaml, '_quarto-singlefile.yml')
}

make_simpleyml(rstudioapi::documentPath())
```

```{r}
#| label: packages
#| include: false

#renv::install("~/Georgia/HydroBOT")
library(HydroBOT) 
library(dplyr)
library(sf)
library(huxtable)
library(ggplot2)
library(patchwork)
```

```{r}
#| label: directories
#| include: false

# This depends on the same scenarios as the demo website, so give it the path to that. This will likely be user-specific. Everything else is relative.

#demo_webdir <- file.path('../WERP_toolkit_demo')
demo_webdir <- file.path('~', '../Deakin University/QAEL - WERP in house - WERP/Toolkit/Writing/Demonstration paper')

# Why is the execute-dir not working?
# Outer directory 
project_dir = file.path(demo_webdir, 'demo_scenarios')  # '..', 

# Hydrographs
hydro_dir = file.path(project_dir, 'hydrographs')  

# EWR outputs
ewr_results <- file.path(project_dir, 'module_output', 'EWR')  

# outputs of aggregator
agg_results <- file.path(project_dir, 'aggregator_output', 'sdl_target') 

# outputs of aggregator
agg_results_gauge <- file.path(project_dir, 'aggregator_output', 'gauge_target') 

```

```{r}
#| label: data-subsets
#| include: false

gauges_to_plot <- c('412002', '419001')#, '422028', '421001')

scenarios_to_plot <- c("climatedown2adapt0", 
                       "climatedown2adapt250",
                      "climatedown2adapt6500",
                      "climatebaseadapt0",
                      "climatebaseadapt250", 
                      "climatebaseadapt6500",
                      "climateup2adapt0",
                      "climateup2adapt250",
                      "climateup2adapt6500")

scenarios_to_plot2 <- c("climatedown2adapt0", 
                      "climatebaseadapt0",
                      "climateup2adapt0")
```

```{r}
#| label: scenario-info
#| include: false
print(file.path(hydro_dir,'scenario_metadata.yml'))

scenarios <- yaml::read_yaml(file.path(hydro_dir,                                     
                                       'scenario_metadata.yml'))


```

```{r}
#| label: scenario-info2
#| include: false
scenarios <- scenarios |>  
  tibble::as_tibble() |> 
  dplyr::rename('scenario' = "scenario_name")

# Add Georgia's scenario codes
scenarios <- scenarios |> 
  arrange(flow_addition, flow_multiplier) |> 
  group_by(flow_addition) |>
  mutate(climate_code = LETTERS[1:n()]) |>
  ungroup() |> 
  group_by(flow_multiplier) |> 
  mutate(adapt_code = 1:n()) |> 
  ungroup()

# set a sceneorder
sceneorder <- forcats::fct_reorder(scenarios$scenario,
                                   (scenarios$flow_multiplier +
                                      scenarios$flow_addition/100000))

# But we usually use the codes, so we need to order them too.
rename_sceneorder <- scenarios  |> 
  mutate(scenario = ifelse(scenario == 'MAX', 'MAX', 
                           paste0(climate_code, adapt_code))) |> 
  pull(scenario) |> 
  forcats::fct_reorder((scenarios$flow_multiplier +
                          scenarios$flow_addition/100000))
```

```{r}
#| label: data-import
#| include: false

# Now that the data is in, deal with the extra junk associated with unique scenario names, hence the str_remove_all

#Hydrographs- just read in the ones we use
scenehydros <- read_hydro(hydro_dir, 
                          scenariofilter = stringr::str_c(scenarios_to_plot, '_', scenarios_to_plot), 
                          long = TRUE, format = 'csv') |> 
  mutate(scenario = stringr::str_remove_all(scenario, '.*0_')) |> 
  left_join(scenarios, by = 'scenario') |>
  rename(gauge_flow = gauge)|>
  tidyr::separate(gauge_flow, into = c("gauge", NA), sep = "_", remove = FALSE)

#Agg data (1.2 GB)
agged_data <- readRDS(file.path(agg_results, 'achievement_aggregated.rds')) |>
  purrr::map(\(x) mutate(x, 
                         scenario = stringr::str_remove_all(scenario, '.*0_'))) |> 
  purrr::map(\(x) left_join(x, scenarios, by = 'scenario'))

# because we use '_' for the 1.0 etc, it messes up some of the directories. Clean that up
agged_data <- purrr::map(agged_data,
                         \(x) dplyr::mutate(x, 
                                            scenario = stringr::str_remove_all(scenario, '_clim.*')))


#Agg data (1.2 GB)
agged_data_gauge <- readRDS(file.path(agg_results_gauge, 'achievement_aggregated.rds')) |>
  purrr::map(\(x) mutate(x, 
                         scenario = stringr::str_remove_all(scenario, '.*0_'))) |>
  purrr::map(\(x) left_join(x, scenarios, by = 'scenario'))

# because we use '_' for the 1.0 etc, it messes up some of the directories. Clean that up
agged_data_gauge <- purrr::map(agged_data_gauge,
                         \(x) dplyr::mutate(x, 
                                            scenario = stringr::str_remove_all(scenario, '_clim.*')))

```

```{r}
#| label: flow-differences


# To get a single dimension for the scenarios, we can use the overall difference in flow volumes. It's crude, but likely the only thing that works when we have an addition over part of the year and a multiplicative change. Those are fundamentally different, so we'll do our best

dif_flow <- baseline_compare(scenehydros, compare_col = 'scenario',                                             base_lev = "climatebaseadapt0",
                             values_col = 'flow',                               
                             comp_fun = c("difference"),
                             group_cols = c('Date', 'gauge')) |> 
  group_by(scenario) |> 
  summarise(scenario_difference = mean(difference_flow)*0.001)

scenarios <- left_join(scenarios, dif_flow, by = 'scenario')

```

```{r}
#| label: palettes
#| include: false

# Qualitative
SDL_pal <- make_pal(unique(agged_data$sdl_units$SWSDLName), 
                    palette = "impressionist.colors::la_recolte_des_foins_eragny")

gauge_pal <- make_pal(unique(gauges_to_plot),                       
                      palette = 'ggsci::nrc_npg')

adapt_pal <- make_pal(as.character(unique(scenarios$adapt_code)),
                      palette = 'nationalparkcolors::Redwoods')

# descriptive networks
net_pal <- list(NodeType = 'nationalparkcolors::MtRainier')

# these have to use quantitative even though they're not, or we run out of colors.
env_pals = list(EB = 'grDevices::Grays',
  EF = 'grDevices::Purp',
                NF = 'grDevices::Mint',
                NV = 'grDevices::Burg',
                OS = 'grDevices::Blues 2',
                WB = 'grDevices::Peach')

# easier printing if they have the class
make_colors <- function(x){
  class(x) <- 'colors'
  return(x)
  }

# use the first level of each of those to make a named pal. I wish there were an easier way
envgroup_pal <- purrr::imap_chr(env_pals,
                                \(x,y) make_pal(levels = y, palette = x)) 


Target_pal <- stats::setNames(envgroup_pal, 
            c(NA, 'Priority ecosystem function', 'Native fish', 'Native vegetation', 'Other species', 'Waterbirds')) |> 
  make_colors()


# Quantitative- sequential achievement
achieve_pal <- 'grDevices::Blue-Yellow'

# quantitative- diverging achievement (e.g. relative to baseline)
compare_pal <- 'scico::bam'

# We don't end up using scene_pal, I don't think. 
# But I also think we should
# Since there are two dimensions, maybe use faded colors? Should be able to bring that function over.
# But do that later
scene_pal <- make_pal(unique(scenehydros$scenario),                       
                      palette = "viridis::mako", #'ggsci::nrc_npg', 
                      refvals = 'base', refcols = 'black')


climate_code_pal <- stats::setNames(scene_pal[c(1, 4, 7)],
                                    unique(scenehydros$climate_code))

sceneTarget_pal <- stats::setNames(c(envgroup_pal, climate_code_pal), 
            c(NA, "Priority ecological function", 'Native fish', 'Native vegetation', 'Other species', 'Waterbirds', names(climate_code_pal))) |> 
  make_colors()

#Target Palette
sceneTarget2_pal <-  stats::setNames(c("#5F9776", "#5F9776", "#5F9776", "#5F9776", "#5F9776", "#4873B8", "#B7A447", "#FDC010", "#f47f51", "#2e3d5a", "#bf3729"), 
            c("Native fish", "Native vegetation", "Waterbirds", "Other species", "Priority ecosystem function", "End of system flows", "Water allocation", "Agricultural benefits", "Social benefits", "I", "E")) |> 
  make_colors()

sceneTarget2_pal
```

# Highlights

The HydroBOT toolkit for data and model integration and synthesis to assess outcomes for disparate target values.
Scaling and synthesis of results across space, time, and groups of values.
Comparison of outcomes resulting from climate change and the impacts of adaptation options.
Automation, transparency, and error detection for consistent repeatable analyses.

# Graphical abstract

![](~/../Deakin%20University/QAEL%20-%20WERP%20in%20house%20-%20WERP/Toolkit/Writing/Demonstration%20paper/images/GraphicalAbstract-01.png){#fig-graphab width="16cm"}

# TODO

FIND.REPLACE for consistent terminology broader ecological values OR environmental targets ?? ecological objective OR ecological objective flow OR water availability

And 'Aggregator/Aggregation'- can we call this 'onward modeling' or at least 'scaling'?

FIND OUR NAMES- there are to-dos scattered around.

historical base level

# Abstract

Water management rarely manages only for water; instead, management targets are the myriad values across ecological, social, and economic outcomes that depend on that water. To assess past performance and plan future action, water managers need tools to understand how changes to hydrology (over which they typically have most control) affect these water-dependent values. Climate change accentuates the need to understand how values might shift and assess potential options for avoiding, ameliorating, and adapting to those impacts. However, models relating values to hydrology and management levers can be difficult to usefully integrate into management processes. These models are often developed for other uses, such as research, and are not easily integrated across values or types of values. Not only is the sheer number of potential values difficult to synthesize into management-relevant conclusions, but economic and ecological models might differ in language, approach, goals, scale, *et cetera*. Here, we describe and demonstrate HydroBOT (Hydrology-dependent Basin Outcomes Toolkit), a holistic modelling toolkit co-designed from the ground up with the Murray-Darling Basin Authority (the primary federal water management agency in the Murray-Darling Basin, Australia). HydroBOT uses a modular, open-source architecture to integrate disparate models of response to hydrology into a consistent, repeatable framework. It then provides capacity to scale and synthesize those results across space, time, and groups of values, targeting outputs relevant to the water managers. A key feature of HydroBOT is flexibility with guardrails; it allows users to tailor analyses and syntheses to a range of questions, from local, short-term evaluation to longer-term basin-scale assessments of climate response across thousands of values. HydroBOT provides fundamentally new capacity to move beyond hydrology to assess outcomes across a range of target values, and its modularity and flexibility provides capacity for continual upgrades and improvements as new models are developed and new needs are identified.

# Key words

1) Environmental flows; 2) Water management; 3) Hydrology; 4) Toolkit; 5) R package; 6) Synthesis

# Software availability

Software name: HydroBOT (Hydrology-dependent Basin Outcomes Toolkit).
Developer: QAEL (Quantitative Aquatic Ecosystem Laboratory) led by Galen Holt.
First year available: 2023.
Program language: R, Python.
Program size: 22 MB.
Operation system: Linux, macOS, Windows.
Availability: https://github.com/MDBAuth/HydroBOT
License: MIT

# Introduction

Water management in large river systems typically targets a wide range of desired values. For example, in the Murray-Darling Basin (MDB), Australia, the objective of water management is to maintain a healthy working river that supports productive and resilient water-dependent industries, healthy and resilient ecosystems, and communities with access to sufficient and reliable water supplies [@murray-darlingbasinauthority2011]. These values, defined here as any social, economic, environmental or cultural asset or function of significance, importance, worth, or use, are not unique to Australia; water is managed to protect these values worldwide [e.g. @stern2019; @kaye-blake2014; @ziolkowska2016; @connor2015]. That these values are the target of water management implies that each value depends on water in some way but, in many cases, these dependencies are not well-defined and management proceeds under the relatively simple assumption that, if water is provided, these values will be maintained.

Water managers typically have a limited range of levers at their disposal which primarily affect hydrology, e.g. flow releases from dams or inundation of wetlands. When non-hydrologic levers are available, such as water trading rules in the Murray-Darling Basin, their impact is still often assessed by how they alter hydrologic conditions-- their impact on the spatio-temporal patterns of flow in the system. Further, hydrologic modelling is often better-integrated with management workflows than models of other values. In part, this is due to the availability of large-scale physical models that provide robust ability to model flows as they arise from natural drivers, such as rainfall, and capture infrastructure such as dams and diversions. While complex, these models have relatively high precision and relatively few outcomes compared to models of ecological or economic responses, for example. The impact of particular management actions (e.g. dam releases) on hydrology is well-understood and captured by the hydrograph, allowing accurate targeting of particular aspects of the flow regime by management actions (e.g. [@singer2007; @loire2021]). While these proximate models of system state and impact of management actions are invaluable, they do not provide the necessary information to assess the status of the range of flow-dependent values.

Assessing values as they respond to flow and management actions requires models of these relationships. Such models exist, though their quality and extent vary widely, ranging from unstated mental models to highly-detailed population dynamics or economic models [@lester2019]. When these models are quantitative or computational, they are written in various languages by subject-matter experts, often focus on one or a limited suite of *GEORGIA* target values and return disparate outputs depending on their particular goals and approaches. These models are often designed with the goal of studying the dynamics of highly specific variables (e.g. fish population responses to water management regimes), not to produce the most useful analysis for management questions or to capture the breadth of target values. Moreover, these models cannot individually provide information about larger-scale values; single-species models are insufficient to assess the condition of all fish or the whole ecosystem, for example. Integrating these models into a holistic modelling approach is necessary to assess management-relevant outcomes of disparate, typically broad-scale, values under different hydrologic conditions and management actions. Integrating diverse response models gives each response model utility beyond its original purpose and identifies where new work is needed (e.g. where response models are limited or non-existent). The creation of an integrated toolkit provides the opportunity for robust decision-making [@harrison2023] and the ability to prioritize water planning across a range of values and identify conditions that achieve disproportionately large (or small) impacts, as well as the explicit consideration of synergies and trade-offs among different values.*These first few paras probably need more general cites in them GEORGIA*

Exemplifying these issues, the Murray-Darling Basin Authority (MDBA) is a federal body charged with water management in the MDB, Australia. The Murray-Darling Basin is centrally important to the economic, cultural, social and environmental well being of Australia. Within the MDB, 70% of water is utilized for agricultural purposes. The MDB contributes nearly 2% to gross domestic product from agriculture and tourism, is home to 2.2 million people, including more than 40 Aboriginal Nations, and contains approximately 400,000 water-dependent ecosystems [@hart2021; @brooks2021]. While balancing this range of values is difficult in any highly-utilized basin, the Murray-Darling is unusually dry and variable compared to other systems of similar importance [@hart2021]. Modern management of the MDB arises from the Commonwealth [@wateract2007], which established the Murray-Darling Basin Authority with obligations to develop and implement a Basin Plan governing water resource management [@hart2021] *I'm citing Hart a lot because it's a big overview, but are there better cites? Something internal?*.

The MDBA has obligations to manage water to maintain a healthy working river and the state of the MDB is required to be assessed annually, with a major review of, and updates to, the Basin Plan made at legislated intervals [e.g. 15 years between development and first review, @hart2021]. Beyond these requirements, the MDBA has an increasing focus on assessing system response to climate change and possible adaptation actions that may be taken by the MDBA itself or other stakeholders in the MDB [@neave2015]. In addition to the MDBA, Basin states and other stakeholders (e.g. the Commonwealth Environmental Water Holder) have additional responsibilities for water management in the MDB [@hart2021] *Is there a better cite for who has control over what?*. Taken together, these management goals require a holistic modelling approach that integrates across values and is adaptable to these various management needs while also simplifying repeated, ongoing use. These needs are not unique to the MDBA and other water management groups, researchers and stakeholders are likely to have similar complex and competing needs.

The MDBA has access to robust hydrologic models describing flows in the system and responses to current management practices, and these models are under continual use, development, and improvement ( *What's the best cite? David, Martin?*). Despite the need for ongoing assessment and forward planning across a range of values, models of the responses to those hydrologic conditions are patchy, only represent some values, and have been developed and used in a more ad-hoc approach rather than integrated with each other or the hydrologic models. In this paper, we describe an integrated modelling 'toolkit' to address these water management needs in the Murray-Darling Basin, Australia, hereafter referred to as *HydroBOT* (Hydrology-dependent Basin Outcomes Toolkit).

The toolkit described here greatly improves the capacity of the relevant manager (here, the MDBA) to assess outcomes across a range of values, provides the structure to adapt and include additional values as additional component models become available, and addresses a number of issues with current assessment practices and modelling approaches. By providing a single, consistent interface to a range of response models, we avoid the need to manually run models separately and we abstract their different interfaces, languages, and idiosyncrasies. Moreover, the toolkit design is highly modular, built to allow the integration of new response models with limited additional updates to HydroBOT itself. Continuing with this consistency, HydroBOT provides a standard set of synthesis approaches and functions that target management-relevant analysis and interpretation of outcomes from these disparate models with a common approach and design language. Because the need for assessing outcomes is nearly universal in water management, and not limited to a particular scenario or project, HydroBOT is designed with strong scenario-comparison capabilities but is agnostic to what those scenarios represent. For a similar reason, the standardised synthesis and outputs are highly flexible, allowing the user to choose the most relevant outputs for different management needs.

One common issue with modelling in general, and particularly integrated models spanning several tools, is that such tools can become a black box where the relationships modelled are opaque. This can yield mistrust by the public and other stakeholders influenced by the model, but also mistrust and misunderstanding by users and developers of the model. To avoid these issues, HydroBOT has been continuously co-designed with the relevant management agency, the MDBA, and an emphasis is put on public code, reproducibility, self-documentation and production of outputs that describe the model itself in addition to its results (e.g. causal relationships).

To demonstrate HydroBOT, we develop a set of example scenarios capturing some qualitative aspects of one intended use: the assessment and comparison of outcomes under different climate scenarios and different adaptations to those changes. These axes represent changes to the system due to processes over which managers have no control (i.e. a changing climate), and management actions which would typically be targeted interventions. We use this demonstration to illustrate the problem space and show how the toolkit can aid in assessing potential system change and prioritising management actions to mitigate impacts.

# Methods

The purpose of HydroBOT is to assess how hydrographs representing various climate and adaptation scenarios affect multiple sets of values, which may span many scales and disciplines. We seek to make these assessments in a consistent and repeatable way despite differences in the response models. HydroBOT then has the capability to synthesize a wide range of outputs from the response models into results that compare scenarios and are digestible and useful for management decision making (@fig-tktomgmt). A co-design process including scientists, software developers, and managers was developed to ensure the toolkit achieved its goal of producing scientifically robust results that are also management-relevant.

![Conceptualisation of HydroBOT architecture (blue arrows) and benefits to management (yellow arrows). Scenarios, represented by hydrographs, reflect observed flows or modeled scenarios (produced via hydrological modelling outside HydroBOT). These hydrographs are fed in a consistent way to various response models (see ) via the HydroBOT Controller (see ). Additional inputs of spatial data (see ) and causal networks (see ) provide grouping information for scalling via the HydroBOT Aggregator (see ). the HydroBOT Comparer synthesizes the results into management-relevant outputs that aid decision-making (see ). This decision-making process is also supported by feedback from stakeholders, other expert advice and the social and political landscape. Water planning and policy decisions can then guide the development of scenarios to assess potential adaptaion options, which in turn can be run through the HydroBOT archetecture.](~/../Deakin%20University/QAEL%20-%20WERP%20in%20house%20-%20WERP/Toolkit/Writing/Demonstration%20paper/images/Conceptual_fig_demopaper2.png){#fig-tktomgmt width="16cm"}{width = 16cm}

## Co-design and scoping for management relevance

Avoiding HydroBOT becoming a 'black box' builds trust, interpretability and usability of outputs created with the toolkit. To ensure HydroBOT meets management needs and is trusted by management users, a toolkit development team was created consisting of primary toolkit developers, hydrologic modelers, and MDBA staff. Collaboration among this team ensured decisions about toolkit construction reflected the needs of the end-user. As implementation proceeded, the collaboration provided a mechanism by which goals could be adjusted or the implications of decisions discussed and understood. By having this window into the toolkit development, the managers obtained a much more granular view of how it operates, and its capabilities and limitations.

Key to the collaborative development was identification of the response models to include. These models need to provide information about values relevant to water management decisions as those values respond to hydrology. Early in the development process, a wide scan was taken to identify candidate response models. This scan considered models being used or developed within the MDBA itself, other government agencies (both federal and state), and externally. This scan found that there were a number of response models available, but many were outdated or highly manual, and their use was patchy both within and across management agencies [@holt2022]. Ultimately, only one was suitably modern and well-developed to include in HydroBOT, the Environmental Water Response (EWR) Tool . Other in-development modules were identified as candidates to include during the life of the toolkit, including economic and social models. Further, this scan identified values that are mandated management targets for which there were no available or in-development response models, highlighting areas needing future work.

## Toolkit overview

A key goal of HydroBOT is to provide a flexible platform for comparing outcomes between different possible scenarios. Scenario comparison is essential for use in both evaluating past actions or conditions and planning for the future, whether to better understand potential shifts in the system from external forces (e.g. climate change) or to assess potential management actions. However it can be difficult to collapse a multitude of scenarios into a format that is easily digested by decision-makers. Further, this process often needs to be supported by feedback from stakeholders, other expert advice and the social and political landscape. Accordingly, a flexible architecture with distinct components that can be run separately (modular) or together can assist with the iterative nature of decisions-making for Water planning and policy.

### Architecture

Taken holistically, the architecture of the toolkit comprises five primary components and the links between them (@fig-tktomgmt). The data flow through the toolkit is represented by the boxes along the top of @fig-tktomgmt, and in more detail in the Supplement (@fig-architecture). Input data (hydrographs) is ingested by the Controller, which packages and runs Response Models. Results from the Response Models are then processed by the Aggregator, and the analysis and final outputs are prepared by the Comparer. The two components outside this data flow are the Causal networks and Spatial units, which are used within the Aggregator and Comparer to define groupings for aggregation and for visual representation of space and the complex inter-relationships defined in the response models. Each of these components is described in more detail below.

HydroBOT provides the tools and capacity to integrate disparate response models, which may be developed elsewhere, typically by subject-matter experts or for other purposes. These may include Causal networks describing the dependencies and outcomes from those models. Thus, the Response models and Causal networks are both depicted as defined externally to the toolkit (@fig-tktomgmt; @fig-architecture) , and new response models require toolkit development to integrate them into a compatible, modular toolkit components. Spatial units (typically polygons of management interest) also originate elsewhere and some are provided by HydroBOT with light changes for optimisation.

The architecture of the toolkit emphasizes modularity. Each of the components can save its outputs, allowing users to run either the whole toolkit or re-run needed components only to update analysis. For example, if a user wished to change how they aggregate the results of the modules, they could re-run the aggregation step and the subsequent comparison step to match. This ability to adjust intermediate steps allows rapid iteration of results to address a given management question, or adaptation of preexisting results to new questions. Modularity also allows rapid, iterative development of the toolkit itself. Any of the components of the toolkit can be updated without affecting others, and so obtaining new results from updated components (e.g. new aggregation capability) simply requires re-running the toolkit from that point forward.

As each stage of the toolkit runs, it produces yaml and json metadata files including the run settings and other attached information relating to the scenarios to enhance repeatability, ensure correctness and increase comprehension. These metadata include all parameters for the run, allowing exactly repeatable analyses to be conducted by using the metadata files as parameter files for subsequent runs. Even if a run is started in the middle, such as to re-run aggregation in a different way, the metadata for that run captures the metadata for the previous steps, ensuring that outputs at every stage are tagged with full provenance information about the run that created them.

### Implementation

The toolkit is available as an R package (github.com/MDBAuth/HydroBOT), which provides a suite of functions representing the steps in the architecture. These functions are designed to be general, allowing users much flexibility in how they run the toolkit for a particular set of analyses while retaining a consistent structure and outputs. Because translating from hydrologic scenarios to various responses is a general problem in water management, we expect the ways in which the toolkit is used will be highly variable. Thus, by providing a general structure in the toolkit, users can target the particular questions and particular analyses needed for a given question. For example, in some cases we might want to look at the responses of different components of fish life cycles for a small subset of locations in the MDB. In another situation, we might want a big-picture view of how climate might shift long-term goals for the environment as a whole. Although the initial impetus for creating the toolkit was to assess climate scenarios, its use in practice can be far more general. Any hydrograph can be assessed, and any set of scenarios represented by hydrographs can be compared, provided a response model exists.

The toolkit is designed for analysis of management questions in the Murray-Darling Basin, and so along with the functions to perform the analyses, it also provides a standard set of spatial data comprising the MDB itself, river lines and gauges within the MDB (at which hydrographs may be available), and various management units (Sustainable Diversion Limit units, Resource Plan Areas, and catchment boundaries for major subcatchments). Some example hydrograph data are also included for testing and demonstration. The toolkit provides a clean causal network for included modules, described in more detail in @sec-causal_networks. Analogous elements can be constructed for other spatial units or responses to extend functionality within or outside the Murray-Darling Basin.

As a result of the scan of available response models, toolkit development proceeded with the EWR tool as a single response model, but with an architecture designed to allow modular integration of additional response models as they become available. The scan accentuated a critical feature of this modularity; the toolkit must be able to incorporate and standardise models written in various languages and with a wide range of input needs and outputs. The toolkit achieves this by wrapping those other tools so as to make their differences as hidden from the user as possible. In the case of Python modules, the toolkit uses the {reticulate} R package [@ushey2023] in combination with small amounts of internal Python code to call Python modules directly. The Python code internal to the toolkit performs limited cleaning and translation to prevent passing large objects between languages and ensure that any idiosyncrasies in module inputs and outputs are handled consistently. Python dependencies (and Python itself) are automatically installed on first use of Controller functions that call Python modules unless they already exist on the user's system. This approach provides essentially invisible Python for most users, while providing flexibility for the user to provide their own Python environment if desired. Modules in other languages are not yet available, but the key requirement is that they be available in a format that is scriptable. In that case, the toolkit will provide small setup and cleanup functions as with the Python modules and wrapper functions to call these modules.

The modularity of the toolkit means it can be run stepwise, with the user calling the relevant functions at each step ([@fig-architecture]). The outputs of each step can be saved or returned directly to an interactive session or both. Typically, sufficient selected outputs would be saved for reproducibility and speed unless the project is small enough to retain a comprehensive set in memory or re-run quickly in a notebook. There are also wrapper functions provided that allow running the entire toolkit from Controller through Aggregator, which are extremely useful for large runs or remote runs on batching services. One particularly useful wrapper provides the ability to run from a yaml config file providing function arguments. This function allows the use of a default file and a 'modified run' file, making it ideal for holding many parameters constant at a default for a particular set of analyses and only changing some on a per-run basis in a smaller file. It also takes command-line or R list arguments, making it a flexible solution to run the toolkit from the command line, in scripting contexts, or Quarto notebooks. The metadata saved at each step in the process is a yaml file with parameters that are a superset of those needed to run the toolkit. Thus, an exactly identical run can be produced by running the toolkit using an output metadata file as an input parameter file. These metadata files also include the git hash, further allowing reproducibility and documenting provenance in the face of code changes.

In practice, the toolkit functions are primarily run in one of two ways. Interactive investigation of relatively small sets of hydrographs can be done in notebooks (typically Quarto; @allaire2022) or simple R scripts. Larger investigations typically would be run on remote computers as part of batching systems, whether HPC, Azure, AWS, or other, with the outputs at the end of the aggregation (and potentially each step) returned. The Comparer step would usually not be run as part of this larger batching, but considered interactively for two reasons. First, through the aggregation step, all operations can proceed in parallel over scenarios, and in some cases parallelizing over gauges is possible. The Comparer necessarily looks across scenarios, and so breaks this parallelisation. Conducting comparisons interactively is typically not excessively CPU- or memory-intensive, provided the Aggregator step has been well thought through. If there are enough data to require high processing or memory, it is unlikely to be simplified enough to make interpretable figures. Second, the primary goal of the Comparer is to produce usable, interpretable outputs. Arriving at a set of meaningful outputs is an iterative process that rarely will be precisely known *a priori*, and so working through this step interactively is necessary. If, however, the toolkit is being used for ongoing monitoring of the same analyses (e.g. a 'dashboard'), then the first iteration may be done interactively, with subsequent uses incorporating those settings written into a script to auto-generate the same figures.

## Toolkit components

Here we describe the specific implementation of each component of the toolkit [@fig-tktomgmt], with additional detail in [@sec-appendix1]. Each component of the HydroBOT toolkit is distinct, allowing modular changes to be made without altering the function of other components.

### Controller {#sec-controller}

The 'Controller' component of the toolkit is the interface between the externally generated input data (scenarios), the chosen response model, and other external and internal components of the toolkit (Appendix @fig-architecture). This component initiates the downstream processing steps according to user-defined settings for a particular run. It includes arguments for locating the input data and the response model(s) to use along with any necessary parameters for those models. The Controller can also control later components of the toolkit, allowing the full toolkit to be run at once. These include defining aggregation steps as discussed in @sec-aggregator and analysis of the results with the Comparer ( @sec-comparer ). The Controller determines whether and where outputs are returned at each step. Having control over the full toolkit process enables large batched runs using parameter files to specify the control arguments in yaml files. This core functionality of the Controller is delivered via simple functions that apply to the input data for each scenario and can be looped over scenarios in parallel. The controller can be accessed by the user by using Quarto notebooks to work interactively, R scripts, or via the command line, depending on the use case.

### Causal networks {#sec-causal_networks}

Causal networks are models that describe the topology of dependence among many drivers and outcomes of different type [@peeters2022; @martínez2019]. The use of a causal network framing both at a high level for visualisation and communication with the MDBA and embedded in HydroBOT illustrate both the goals and the functioning of the toolkit, avoiding it becoming a black box. The relevant causal network for water management in the Murray-Darling Basin captures relationships between management, flow, and outcomes for a wide range of values, from ecological response to economic performance and human wellbeing. Thus, they include climatic and management drivers, but also include the causal relationships which form the basis of the response models (e.g. flow timeseries to hydrologic indicators, or potentially to the life history inherent in a population dynamics model). They incorporate any links connecting those outcomes to larger-scale outcomes (e.g. from hydrologic indicators to ecological response). Thus, causal networks define an overarching model from initial inputs (e.g. rainfall or flows or adaptations) through to all values of interest, with each link defining a response model or component of a response model. The specifications of the models underlying each link are highly variable. These models range from detailed physical models linking runoff to flow, to simpler ecological models of environmental water requirements, to simple averages, to leaving the model unspecified where information about the relationships is unavailable. Assessing the quality of knowledge around each link provides a powerful assessment of knowledge deficiencies and uncertainty in responses.

Causal networks themselves, i.e. the structure of the links (relationships) and nodes (state variables) can be derived from many sources, including empirical studies defining the existence of causal relationships and expert opinion. HydroBOT provides a causal network for included modules, where available, to describe how their outputs arise from hydrology and how they relate to various levels of management-relevant outcomes. The causal networks enable: 1) visual representation of the complex inter-relationships between scenario inputs and outcomes across a range of objectives and 2) assessment of outcomes aggregated along the value dimension (see @sec-aggregator). The former aids transparency, elucidating the intentions and causal relationships behind the response models and is a useful device for communication alongside other final outputs. The latter allows outcomes to be quantified for individual (or sets of) objectives (e.g. fish breeding), values (e.g. native fish), or at the overarching levels of environmental, cultural, social, or economic values. This quantification provides a powerful assessment tool and the ability to identify synergies and trade-offs across interrelated values.

### Response models {#sec-modules}

The impacts of climate and adaptation options on social, cultural, environmental, or economic values are estimated based on causal relationships between drivers (e.g. hydrology) and responses (e.g. the state of values). The response models may exist in many different forms, ranging from binary achievement of hydrologic indicators to fully quantitative responses. These tools are expected to be sourced from existing or in-development models developed by subject-matter experts and, as such, will be written in different languages and will target different outcomes. HydroBOT then provides a unified interface and ongoing analysis and modelling of the response model results.

#### EWR (Environmental Water Requirements) {#sec-ewr}

The EWR tool, which forms the core of this demonstration, is one such response model, written in Python and in use by the MDBA internally, the state of New South Wales for water planning and other interested stakeholders. It models the response of ecological values of the Murray-Darling Basin founded on hydrologic indicators paired with causal relationships to ecological values. It holds databases of the environmental water requirements (EWRs; the indicators) and the volume, frequency, timing and duration of flows or inundation required to meet those indicators (@sec-ewr-table). These indicators were developed based on hypothesized relationships to the ecological objectives of the MDB, which protect or enhance environmental assets and ecosystem functions that are valued based on ecological significance [@sheldon2024]. The EWR tool itself only provides an assessment of the hydrologic indicators. The EWR tool assesses whether spatially explicit flow timeseries data meets each EWR (hydrologic indicator) at each gauge (illustrated in @fig-ewr-example). The precise definitions for each EWR differ at each gauge, due to the unique hydrology and channel morphology. For example, a small fresh for 10 days is required every year, ideally between October and April to meet indicator EWR SF1 (small fresh 1), but the flow volume defined as a 'small fresh' differs between gauges. For HydroBOT to model environmental outcomes, we connect the EWR tool results to a specific causal network defining these links.

```{r}
#| label: fig-ewr-example
#| fig-cap: !expr glue::glue("The magnitude of required EWR are illustrated at two example gauges ({paste0(gauges_to_plot, collapse = ' and ')}) on the historical base level (scenario E1) hydrographs. This does not illustrate the requred frequency, duration or timing or EWR; see @sec-ewr-table for a table.")

# This relies on the NSW ewr definitions. We could keep passing them around, but better to pull from py-ewr directly (now a handy toolkit function)
ewrs_in_pyewr <- get_ewr_table()

NSWEWR_gauge <- filter(ewrs_in_pyewr, Gauge %in% gauges_to_plot) |>
  tidyr::separate_wider_delim(cols = Code, delim = '_', 
                              names = c("Code", "Timing"),
                              too_few = 'align_start',
                              too_many = 'merge') |>
  rename(gauge = Gauge) |>
  group_by(gauge) %>%
  distinct(Code, .keep_all = TRUE) |>
  mutate(FlowThresholdMin = as.numeric(FlowThresholdMin)) |>
  arrange(FlowThresholdMin) %>%
  mutate(Date = seq(from = max(scenehydros$Date), 
                    to = min(scenehydros$Date), 
                    length.out = n())) %>%
  ungroup()

hydro_plot_EWRs <- scenehydros |>
    dplyr::filter(scenario == 'climatebaseadapt0' & gauge %in% gauges_to_plot) |>
    ggplot2::ggplot(ggplot2::aes(x = Date, y = (flow/1000) + 1)) +
    ggplot2::geom_hline(data = NSWEWR_gauge, 
                        mapping = aes(yintercept = (FlowThresholdMin/1000) + 1),
                        colour = "red")+ #
    ggplot2::geom_line() +
    ggplot2::facet_grid(gauge ~ . , scales = 'free') +
    ggplot2::labs(y = paste0("Flow (GL/day +1)")) +
    theme_werp_toolkit(legend.position = "bottom") +
      guides(colour=guide_legend(nrow=2,byrow=TRUE)) +
    ggplot2::geom_label(data = NSWEWR_gauge, 
                        mapping = aes(x = Date, y = (FlowThresholdMin/1000)+1,
                                      label = Code), size = 3, colour = "black") +
  # tempted to use 'pseudo_log' and not add 1, but the ticks aren't as nice
  ggplot2::scale_y_continuous(trans = 'log10',
                              sec.axis = sec_axis(~ . , name = "Gauge ID", 
                                                  breaks = NULL, labels = NULL))

hydro_plot_EWRs
```

The EWR tool results are binary responses of hydrologic indicators and so, to link these to expected ecological responses, we extracted and cleaned causal networks from the relationships implied in the Long-Term Watering Plans in setting the EWRs [e.g. @nswdepartmentofplanningandenvironment2020]. These plans describe how the hydrologic indicators are expected to influence both proximate and larger-scale ecological outcomes. We extracted these causal relationships from the Murray-Darling Basin Long Term Watering Plans (LTWPs), which were developed based on the best available information from water managers, ecologists, scientific publications, and analysis of gauged and modelled flows [e.g. @nswdepartmentofplanningandenvironment2020, @lobegeiger2022]. This provided a dense network of links across a range of ecological outcomes at various scales, from the hydrologic indicators to components of the life cycle of single species to whole-community outcomes at 10- or 20-year target dates. For example, for the SF1 indicator described above might contribute to multiple ecological objectives pertaining to native fish (ecological objectives NF1-9; e.g. NF1 = No loss of native fish species), native vegetation (NV1), and ecosystem functions (EF1-5). These ecological objectives are defined to support the completion of all elements of a life cycle of an organism or group of organisms (taxonomic or spatial) to achieve a goal state, condition or characteristic of an environmental asset or ecosystem function" [@nswdepartmentofplanningandenvironment2020]. Outcomes for ecological objectives are then linked in the causal network to five ecological values [native fish, native vegetation, waterbirds, other species, and priority ecosystem functions, @murray-darlingbasinauthority2019] and are associated with long-term targets (5, 10, and 20 year) of the LTWP's management strategies. The chain of dependencies from EWRs to ecological objectives to long-term targets are captured in the causal networks as links, with nodes defining the ecological objectives or long-term targets (@fig-causal-example). This structure not only provides a visual definition of the links in the LTWP, it also enables assessment of outcomes in direct equivalence to the LTWP's management strategies.

```{r}
#| label: fig-causal-example
#| fig-cap: !expr glue::glue("a) HydroBOT incorporates causal networks that describe the ecological objectives for a system. In the current example these causal networks are extracted from the Murray-Darling Basin Long Term Watering Plans (LTWPs), which sets environmental watering requirements (EWRs), ecological objectives, and long-term targets for key water-dependent plants, waterbirds, fish and ecological functions. From left to right, the columns here represent EWR codes, ecological objectives, ecological values, and 5-year management targets. The network shown here is for a single gauge ({gauges_to_plot[1]}), which is the scale at which EWR codes and ecological objectives are defined. The other levels are defined at larger spatial scales, but only those that apply to the EWRs present at this gauge are shown here. Colors represent the log of the relative change in condition from the baseline for the halving (panel a, scenario A) 'climate' scenarios. Large reductions in condition are dark purple, large improvements are dark green, and no change is white. See Appendix 3 *sec-causalnetwork-versions* for more detailed causal network diagrams.")  

aggNetworkdown_sub_rel_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkdown_sub_rel.png')) |>
  grid::rasterGrob(interpolate = TRUE)
# aggNetworkup_sub_rel_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkup_sub_rel.png')) |>
#   grid::rasterGrob(interpolate = TRUE)
net_legend <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkup_sub_legend.png')) |>
  grid::rasterGrob(interpolate = TRUE)

patchwork::wrap_plots(aggNetworkdown_sub_rel_grob) + inset_element(net_legend, 0.90, 0.60, 1, 1)
                       

# patchwork::wrap_plots(aggNetworkdown_sub_rel_grob, 
#                       aggNetworkup_sub_rel_grob)+
#                       net_legend,
#                       #ncol = 2, widths = c(9, 1), byrow = FALSE) +
#   plot_annotation(tag_levels = list(c('a', 'b', 'c'))) +
#   plot_layout(nrow = 3)


```

#### Module standardization

Each response model will have a distinct set of outputs, reflecting the captured responses and the structure of the model. When run within HydroBOT, these outputs are cleaned and processed into standard, expected formats for further toolkit processing, and metadata is saved. This enables HydroBOT to provide a consistent, unified home for disparate response models. The outputs can be saved to disk or retained in-memory for interactive use, depending on the user's needs. The outcomes of the response models are then processed by the Aggregator to provide outcomes at scales relevant for management decisionmaking (in time, space or value dimensions; @fig-aggregation-dims).

### Aggregator {#sec-aggregator}

Results from the response models are typically very granular in many dimensions because the best response models operate near the scale of the processes being modelled. In many cases, those processes (e.g. fish breeding, crop planting) individually occur at small spatial and temporal scales. Note that this is the scale of the process itself, but that may be repeated over much larger scales. For example, fish may breed across the MDB but each female breeds in only one location at a given time. Moreover, outcomes from response models are typically at small value scales as well, e.g. capturing portions of the life cycle of fish species, rather than an overall outcome for all fish, or representing planting of particular crops, rather than overall agricultural output. The consequence is that there are potentially thousands of different modeled outcomes across time, locations and values. This plethora of outcomes must then be aggregated to extract meaning at larger scales and condense the information for digestibility in management decision making.

Aggregation condenses that information to scales that are useful for interpretation and planning. Depending on the use, the desired scale(s) may range from local, short-term responses of fine-grained outcomes to large spatial scales over longer time periods for high-level outcomes such as environmental condition (@fig-aggregation-dims). Thus, HydroBOT must achieve a robust, consistent aggregation approach along three dimensions (time, space, and value), while maintaining the ability to define those aggregation steps flexibly to meet the needs of the specific analysis. For example, the EWR tool assesses EWRs at individual gauges, but outcomes may be desired within areas defined for management purposes (e.g. Sustainable Diversion Limit (SDL) units) or at the basin scale. Likewise, multiple EWRs are required to meet ecological objectives, of which many are required for each ecological value or long-term target (see Appendix 1 for glossary [@sec-glossary]). The HydroBOT Aggregator enables scaling from the hydrology at each gauge to results at any of these scales. HydroBOT provides a standard set of spatial units for aggregation but can accept any user-supplied polygons as the spatial aggregation units ([@fig-aggregation-dims]). Aggregation along the value dimension follows the causal network, which is supplied by HydroBOT for included tools, though the user can specify other causal relationships if required.

![Aggregation along multiple dimensions. HydroBOT provides flexible capability to aggregate along spatial, temporal and value scales. Users can control the sequence of steps along these axes, with the capability to switch between axes at different steps. *TYPO IN SEASONAL- WHERE IS THIS MADE?*](~/../Deakin%20University/QAEL%20-%20WERP%20in%20house%20-%20WERP/Toolkit/Writing/Demonstration%20paper/images/Aggregations.png){#fig-aggregation-dims}

A flexible approach to aggregation is needed as specific dimensions (time, space and value) and units within those are best combined in different ways, depending on the meaning of the data and the use of the final outputs. The choices made in these aggregation steps are critical to ensure the final values are scientifically justified and capture the intended meaning. The Aggregator defines a sequence of aggregation steps to take with the data, specifying the dimensions along which to aggregate and the aggregation functions to apply at each step. Each step can be along any dimension and multiple functions can be assigned at each step. The sequence of aggregation steps can interleave the dimensions, e.g. aggregate along the value dimension to an intermediate level, then aggregate in space, then value again. For example, following @fig-aggregation-dims, the user might want to aggregate from hydrologic indicator (EWR) to ecological objective to specific goal (e.g. species) at each gauge, then aggregate those gauges into a SDL unit to assess performance of each species over a larger area, followed by aggregating to ecological values (broad group, e.g. native fish, waterbirds, native vegetation, ecosystem function).

The Aggregator allows the user to choose any aggregation function at each aggregation step, reflecting the need to account for both the processes being aggregated and the outputs needed for management decision making. These aggregation functions should be considered carefully, as they have different meaning both for the processes being modeled and the interpretation of the outcomes. For example, in some situations the user might want to know the rate at which EWRs pass across some area, or perhaps the average value of an abundance measure. In this case, a mean would be appropriate. In other situations, however, a single failure may be disproportionately important (e.g. 'no loss' requirements), or perhaps a single passing value is sufficient (e.g. bird breeding can occur anywhere in a catchment). These might be captured with a minimum and maximum, respectively. To address this need for flexible aggregation, HydroBOT provides a standard set of functions (e.g. mean, max, min, and spatially-weighted mean), but the user can also specify any other aggregation function, including custom-written functions.

The complexity of the potential aggregation sequences and functions highlights the importance of tracking the provenance of the final values to understand their meaning. Thus, the Aggregator saves the sequences and functions applied at each stage to a metadata file that also can serve as the aggregation parameters for repeatable runs. Moreover, the output dataset retains the full sequence and functions alongside each value, ensuring that values are always paired with their provenance and meaning.

### Comparer {#sec-comparer}

The HydroBOT Comparer is designed primarily to make comparisons between scenarios (typically hydrographs reflecting climate or climate adaptation in the examples that follow), allowing assessment and visualization of their differences. This component also provides generalized capacity to produce plots and other outputs such as tables using a consistent approach, even when not directly comparing scenarios. While the primary use of the comparer is Aggregator output, hydrographs and direct Response model output can also use this functionality.

Comparisons are essential to assess the outcomes of various scenarios, e.g. the behaviour of the system under different climate regimes or with different adaptation options. The functions within the Comparer can be divided into two main categories, those for analysis and those for plotting. Although in some instances presenting the absolute outcome values can be useful across scenarios, explicitly calculating comparisons (e.g. the absolute or relative difference between scenarios) provides distinct advantages. Difficulty in accurately simulating a complex system means that comparisons between scenario outcomes can be more useful and accurate because any bias in the baseline assumptions applies to all outcomes and the focus moves from the total level to the change between scenarios [@holt2024]. The best method for comparing will vary depending on the quantities being compared and the intended use of the comparison, so several common default options (differences, relative change) have been developed along with the flexibility for the user to define alternatives.

The comparison functions provide the ability to choose a baseline level for comparison, which may be one of the scenarios, but also may be a reference dataset or a scalar value. For example, we might want to compare a set of outcomes for climate scenarios to a 'no change' climate scenario, to historical observations, or to a mean value. Output values are calculated relative to the defined baseline using either default functions for the absolute or relative difference, or any other user-supplied function. These functions can be applied to any dataframe, but a major advantage of HydroBOT is including them within the plotting functions. This allows the generation of comparison plots from raw Aggregator outputs without the need for subsequent data calculations by the user and avoids potential errors that arise from repeated or forgotten data transformations in an analysis workflow.

The plotting functions in the Comparer provide capability to present and visualize comparisons using standardized procedures for all outputs within a project. Different purposes require different sets of outputs; for instance, maps are particularly useful for visualizing geographic patterns, tables and graphs typically provide more precise ability to assess impacts on values, and timeseries plots are useful for visualizing climate trajectories. While potential visualizations and comparisons vary widely depending on the intended use, the Comparer standardizes data cleaning and processing for each plot, as well as aesthetics and plotting approaches. This standardization ensures consistency across plot types and through the project, ensuring values plotted are robust and interpretable. Key to this standardization is the internal data cleaning, which allows the raw outputs of the Aggregator and arguments for the comparisons to be provided to the plotting functions. By standardizing data cleaning within the Comparer, we avoid losing information or performing unsupported data manipulations and so ensure the quality and meaning of the outputs. For example, because the plotting is aware of the temporal, spatial, and value dimensions, it identifies accidental overplotting or misleading lumping of data across dimensions.

## Demonstration scenarios

We demonstrate toolkit functionality for ecological values by using the existing EWR (Environmental Water Requirements) tool for the response model (see @sec-modules for a description of the EWR module and its associated causal network). Our input data consists of hypothetical flow timeseries generated from historical hydrographs at `r length(unique(agged_data$ewr_code$gauge))` gauges. These gauges fall within the Lachlan, Macquarie--Castlereagh, and Namoi SDL units of the Murray-Darling Basin, Australia ([@fig-sdl-comparison]A). These SDL units have detailed LTWPs, and so well-specified EWRs and causal networks.

We develop a set of simple scenarios that capture two sorts of changes that may be commonly represented in management analyses. First, we consider scaled flow throughout the period of the hydrograph, representing overall increases or decreases in flow as might occur from large-scale climate patterns. Second, we consider short-duration additions to flow, representing periodic pulses of change as might happen from targeted interventions. Each scenario characterizes all water in the system including natural inflows, extraction, and release of environmental water, yielding a complete hydrograph (@sec-scenarios shows a selected subset). These do not reflect realistic future scenarios but provide an avenue to test and illustrate the capabilities of HydroBOT.

To scale flow, we apply nine flow multipliers, ranging from 0.5 to 2.0, to the historical hydrographs (@tbl-scenarios). We refer to these as 'climate' scenarios, reflecting a common representation where entire hydrographs might shift to represent climate change, though these scenarios are not derived from climate models and do not represent hypothesized climate change. To achieve pulsed change for each of the 'climate' scenarios, four flow additions were applied including 1) no addition (baseline), 2) addition of 250 ML/d, 3) addition of 6500 ML/d, and 4) addition of 12000 ML/d (@tbl-scenarios). These additional flows were added throughout the period of September to December. We refer to these scenarios as 'climate adaptations' because management options are often available in the form of altering water availability for short time periods through mechanisms like water releases, though the options here do not represent proposed actions.

For simplicity for this demonstration, we use arithmetic means for all aggregations except the very first, though we typically present results only after this first step. The first step gives achievement of EWR indicators from a set of differently-lenient sub-indicators. We consider that if an EWR is achieved for any of its possible definitions, it is achieved. We thus use a maximum for that aggregation. Though we use means throughout, their meaning changes depending on the level of aggregation. For simplicity, we assume the value of each outcome represents some 'condition', and the 'condition' at a particular level can be assessed as the mean of the conditions contributing to it. For example, the condition of ecological objectives might simply be the proportion of EWRs that contribute to each objective that are achieved. Then, the condition of ecological values, e.g. native fish, might be the mean condition over all the NF1..n objectives, despite those objectives depending on different numbers and sets of EWRs. This captures the idea that native fish condition improves when the life-cycle components captured by those objectives are met, whether it takes 1 or 10 EWRs to meet an objective.

# Results and discussion

Here, we present example results produced using HydroBOT, the demonstration scenarios (described above in @sec-scenarios) and the EWR tool for the response model. These results demonstrate the flexibility and robustness of the toolkit, which give it the capability to produce rigorous but interpretable and management-relevant results in a reproducible way (all code is available at https://github.com/MDBAuth/toolkit-demo-paper).

```{r}
#| label: simplfy-for-plots
#| include: false

# Create a grouping variable
obj_sdl_to_plot <- agged_data$sdl_units |>
  dplyr::mutate(env_group = stringr::str_extract(env_obj, '^[A-Z]+')) |>
  dplyr::filter(!is.na(env_group)) |>
  dplyr::arrange(env_group, env_obj)

obj_sdl_to_plot <- obj_sdl_to_plot |>
  mutate(Target = case_when(      env_group == "NF" ~ "Native fish",
                                  env_group == "NV" ~ "Native vegetation" ,
                                  env_group == "OS" ~ "Other species",   
                                  env_group == "EF" ~ "Priority ecosystem function", 
                                  env_group == "WB" ~ "Waterbirds",
                                  env_group == NA ~ NA))


# We can use spatial_aggregate directly to go straight to sdl from EWR without first averaging over env_obj or gauges. This gives a pure measure of the proportion ewr met in each sdl without issues of uneven contributions.
agg_ewr_sdl <- agged_data$ewr_code |> 
  spatial_aggregate(to_geo = sdl_units, 
                    groupers = c('scenario', 'climate_code', 'adapt_code'),
                    aggCols = ewr_achieved,
                    funlist = 'ArithmeticMean') |> 
  rename(ewr_achieved = spatial_ArithmeticMean_ewr_achieved)


```

Comparison of the direct outputs from the response model (in this example, pass/fail of EWRs) without any aggregation along the value dimension gives an overview of hydrologic responses at different locations (e.g. gauges) or areas (e.g. SDL units) to different scenarios. Such an assessment can identify areas that may be particularly vulnerable or which potential 'adaptation options' might be most impactful across many outcomes. In @fig-sdl-comparison, we present the EWR outcomes for three climate scenarios (A, E and I; 0.5x, no change and 2x the historical base level, respectively) and three adaptation options (1, 2 and 3; additions of 0, 250 and 6500 ML/d, respectively).

This example aggregates the EWR outputs directly to the SDL unit, and shows that they are likely to be affected by the changes in flow resulting from these changes in 'climate' and application of these 'adaptation options'. However, the scenarios affect each SDL unit differently. The Lachlan has the highest proportion of EWRs achieved under all scenarios and has relatively consistent increases in EWR success with the adaptation options (though the options themselves are not even; 0 to 250 to 6500 ML/d). The Macquarie--Castlereagh is most sensitive to small increases in additional water, with large jumps between 'adaptation' options 1 and 2. In all situations, the 'climate' scenarios have less of an impact than the 'adaptation' scenarios, though neither is reflective of expected change in these dimensions. These outcomes are not yet directly linked to ecological values (priority ecosystem functions and environmental assets) and so each hydrologic indicator (EWR) at each gauge is represented with equal importance, whether it is required for all ecological objectives or just one.

```{r}
#| label: fig-gauges
#| include: false

relevant_gauges <- filter(bom_basin_gauges,
                          gauge %in% unique(scenehydros$gauge))

relevant_sdl_units <- agged_data$sdl_units |>
  distinct(SWSDLName, geometry, .keep_all = TRUE)

relevant_basin_rivers2 <- relevant_sdl_units |>
  rename(sdl_geometry = geometry)|>
  st_join(basin_rivers)
  
relevant_basin_rivers <-  basin_rivers |>
  filter(AusHydroID %in% relevant_basin_rivers2$AusHydroID)

relevant_basin_rivers2 <- NULL

gauge_plot <- relevant_sdl_units |>
           plot_outcomes(outcome_col = 'SWSDLName',
              plot_type = 'map',
              colorset = 'SWSDLName',
              pal_list = SDL_pal,
              #underlay_list = list(list(underlay = basin,
              #                     pal_list = 'azure')),
              overlay_list = list(
                list(overlay = relevant_basin_rivers,
                                        pal_list = 'lightblue1'),
                list(overlay = relevant_gauges,
                                   pal_list = 'firebrick')
                                   )) +
theme_werp_toolkit(axis.ticks = element_blank(), 
                   axis.text=element_blank(),
                   axis.title=element_blank())+
theme(legend.position = "none")+
  scale_x_continuous(limits = c(144, 151.5)) +
  scale_y_continuous(limits = c(-35, -29.7))+
  ggspatial::annotation_scale()

gauge_plot

inset_map <- ggplot(ozmaps::ozmap_country) +
  geom_sf(fill = 'grey20') +              
  geom_sf(data = basin, fill = 'azure') +
  geom_sf(data = agged_data$sdl_units, aes(fill = SWSDLName))+
  theme_werp_toolkit(axis.ticks = element_blank(), 
                     axis.text=element_blank(),
                     axis.title=element_blank())+
  scale_x_continuous(limits = c(113, 153)) +
  scale_y_continuous(limits = c(-43.63, -9.22))+
  scale_fill_manual(values = SDL_pal)+
  theme(legend.position = "none")+
  ggspatial::annotation_scale()



# # Why on earth does this print a dataframe of the corners? Islands
# overview_map <- gauge_plot +
#   patchwork::inset_element(inset_map, left = 0.01, bottom = 0.7,
#                            right = 0.4, top = 1)

```

```{r}
#| label: fig-sdl-comparison
#| fig-cap: (a) Map of the Murray-Darling basin, in eastern Australia. (b) Gauge locations within three SDL units (Macquarie–Castlereagh, Lachlan, and Namoi). Scenario comparison for different Sustainable Diversion Limit areas (a) emphasizing quantititive differences and (b) spatial patterns. The mean proportion of environmental watering requirements (EWRs) that are achieved under each scenario for each SDL unit are shown. Climate scenarios and adaptation options as in @tbl-scenarios.
#| message: false

# The bars
sdl_achieve_bars <- agg_ewr_sdl |> 
  filter(scenario %in% scenarios_to_plot) |> 
  mutate(scenario = paste0(climate_code, adapt_code), # This makes the names clean
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Proportion\nEWR achieved',
                x_col = 'scenario',
                x_lab = 'Climate and adaptation scenario',
                colorset = 'SWSDLName',
                color_lab = '',
                pal_list = SDL_pal |> 
                  setNames(stringr::str_replace(names(SDL_pal), '–', '-\n')),
                position = 'dodge',
                setLimits = c(0,1)) +
  # guides(fill = guide_legend(nrow=2, label.position = 'top')) +
  theme(legend.position = 'bottom')

  # if we really want facetted, this will do it, but I don't think it's needed:  
# facet_col = 'scenario',
# scales = 'free_x',

# The map
sdl_achieve_map <- agg_ewr_sdl |> 
  filter(scenario %in% scenarios_to_plot) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Proportion\nEWR achieved',
                plot_type = 'map',
                facet_row = 'climate_code',
                facet_col = 'adapt_code',
                colorset = 'ewr_achieved',
                pal_list = achieve_pal,
                #underlay_list = list(underlay = basin, underlay_pal = 'azure'),
                setLimits = c(0,1))

sdl_achieve_map <- sdl_achieve_map +
  theme(legend.position = 'bottom', 
        axis.ticks = element_blank(), 
        axis.text = element_blank()) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Climate scenario", 
                                         breaks = NULL, labels = NULL)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Adaptation option",
                                           breaks = NULL, labels = NULL))

wrap_elements(full = inset_map) + wrap_elements(full = gauge_plot) +
wrap_elements(full = sdl_achieve_bars) + wrap_elements(full = sdl_achieve_map) + 
  patchwork::plot_layout(nrow = 2, heights = c(5,10, 10)) & 
  plot_annotation(tag_levels = "a")

```

The core response models may not themselves provide responses to all values of interest, and so we use the causal networks to link these outputs to additional objectives. In the EWR example, this linking is essential to address ecological outcomes and investigate how changes in flow affect values of interest. Thus, we can compare how native fish, native vegetation, waterbirds, other species, and ecosystem function targets are likely to respond to our hypothetical set of climates and adaptation scenarios. We are also able to compare the planning unit areas and the scenarios themselves. While there are many ways to display this information, @fig-obj_in_groups consolidates a large amount of information across scenarios, planning units, and multiple scales of objective. Each set of colours (and row of panels) represents outcomes at the larger ecological value level. Within those, the different shades represent single ecological objectives (e.g. WB1; Maintain the number and type of waterbird species), with the heights of these shades being the proportion of the constituent hydrologic requirements that are met (EWRs). Because the overall bar heights are then a sum of these proportions, we provide a dashed line as reference that shows the situation where all EWRs are met for each ecological objective. This reference is necessary because the number of EWRs contributing to each ecological objective, and the number of ecological objectives vary within each larger ecological value group and spatial unit.

This view provides a useful overview of the impact of different scenarios on the ecological values across space, while also retaining the ability to assess individual ecological objectives. For example, WB1 (Maintain the number and type of waterbird species) and WB2 (Increase total waterbird abundance across all functional groups) are unlikely to be achieved in the Macquarie--Castlereagh under any scenarios, while they are met in the baseline and 2x climate scenarios (2 and 3) in both the Lachlan and Namoi. At the larger ecological value scale, we can see that the effect of halving water (moving from the baseline (2) scenario to the 1 scenario) has a disproportionately greater impact on native fish than doubling water (the 3 scenario) in the Lachlan, while these shifts are more proportional in the Namoi.

```{r}
#| label: fig-obj_in_groups
#| fig-cap: "Proportion of EWRs achieved for broader ecological values, including: priority ecosystem function (EF), native fish (NF), native vegetation (NV), other species (OS) and waterbirds (WB) (Colours & rows). Columns are scaled to illustrate the number of ecological objectives that contribute to each broader ecological value, and shades of colours illustrate the proportion of EWRs that contribute to each ecological objective. I.e. if all EWR contributing to ecological objectives for each ecological values the columns would reach the dashed horizonal lines. This view provides an assessment of the performance of individual ecological objectives, as well as how those contribute to the overall sensitivity of the broader groups. Not all EWR are applicable in all locations and there are not equal number of EWR in each category. There are no other species objectives in the Namoi*GALEN/GEORGIA CHECK THIS ONCE RERUN THORUGH EWR TOOL*. This illustration includes three climate scenarios: A (0.5x), E (historical base level/no change), and I (2x); and three adaptation options: 1 (no adaptation), 2 (+250 ML/d), and 3 (+6500 ML/d)" 
#| message: false
#| warning: false

obj_in_groups <- obj_sdl_to_plot |>
  filter(env_group != 'EB') |> # There's only one, this is just distracting.
  filter(scenario %in% scenarios_to_plot) |> 
  # clean the names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = "EWR achievement\nassessed for individual EWRs (shades)",
                x_col = 'scenario',
                x_lab = "Climate & Adaptation Option Scenario",
                colorgroups = 'env_group',
                colorset = 'env_obj',
                pal_list = env_pals,
                color_lab = "ecological objectives",
                facet_col = 'SWSDLName',
                facet_row = 'Target', 
                sceneorder = rename_sceneorder
                )+
  facet_grid(Target~SWSDLName, scales ="free")

# ADD IN MAX: new way?
maxdata <- obj_sdl_to_plot |> 
  st_drop_geometry() |> 
  filter(env_group != 'EB') |> # There's only one, this is just distracting.
  filter(scenario %in% "MAX") |> 
  # clean the names
  mutate(SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |> 
  summarise(ewr_achieved = sum(ewr_achieved), 
            .by = c(SWSDLName, Target))

obj_in_groups <- obj_in_groups +
  geom_hline(data = maxdata, 
             mapping = aes(yintercept = ewr_achieved),
             linetype = 'dashed')

obj_in_groups


```

```{r}
#| label: build-map-figs
#| include: false
ef_maps <- obj_sdl_to_plot |>
    dplyr::filter(scenario %in% scenarios_to_plot) |>
  # Need to reduce dimensionality
    dplyr::filter(env_group == 'EF' & !(env_obj %in% c('EF3a', 'EF3b'))) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Proportion\nEWR achieved',
                plot_type = 'map',
                colorset = 'ewr_achieved',
                pal_list = achieve_pal,
                facet_row = 'scenario',
                facet_col = 'env_obj',
                underlay_list = list(underlay = basin,
                                     pal_list = 'azure'),
                setLimits = c(0,1)) 

ef_maps <- ef_maps + 
  scale_y_continuous(sec.axis = sec_axis(~ . ,
                                         name = "Climate & Adaptation Option Scenario", 
                                         breaks = NULL, labels = NULL)) +
    scale_x_continuous(sec.axis = sec_axis(~ . , 
                                           name = "Environmental objective",
                                           breaks = NULL, labels = NULL)) +
  theme(axis.ticks = element_blank(), 
                         axis.text=element_blank())

# gauges with blank sdl units
  # because gauges contribute to multiple planning units, reporting them is not a good indicator of condition. However, to show how the aggregation works, we can do something here to get single values for them by using theme aggregate directly and allowing it to average over the multiple rows due to multiple planning units.
agged_gauges <- agged_data$ewr_code |> 
  theme_aggregate(from_theme = 'ewr_code', to_theme = 'env_obj',
                  groupers = c('scenario', 'gauge', 'climate_code', 'adapt_code'),
                  aggCols = 'ewr_achieved',
                  funlist = ArithmeticMean,
                  causal_edges = causal_ewr,
                  auto_ewr_PU = FALSE) |> 
  agg_names_to_cols(aggsequence = 'env_obj', 
                    funsequence = 'ArithmeticMean', 
                    aggCols = 'ewr_achieved')

nf_gauges <- agged_gauges |>
    dplyr::filter(env_obj == 'NF1' & scenario == 'climatebaseadapt0') |> # Need to reduce dimensionality
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code)) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Proportion\nEWR achieved',
                plot_type = 'map',
                colorset = 'ewr_achieved',
                pal_list = achieve_pal,
                underlay_list = list(list(underlay = basin,
                                     pal_list = 'azure'),
                                     list(underlay = sdl_units |> 
                                            filter(SWSDLName %in%
                                                     unique(obj_sdl_to_plot$SWSDLName)),
                                         pal_list = 'gray')),
                setLimits = c(0,1)) +
  theme(axis.ticks = element_blank(), 
                         axis.text=element_blank())

# The 'env_ob' dataset is at the planning unit scale before agg to sdl
nf_PU <- agged_data$env_obj |>
    dplyr::filter(env_obj == 'NF1' & scenario == 'climatebaseadapt0') |> # Need to reduce dimensionality
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code)) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Proportion\nEWR achieved',
                plot_type = 'map',
                colorset = 'ewr_achieved',
                pal_list = achieve_pal,
                map_outlinecolor = NA,
                underlay_list = list(list(underlay = basin,
                                     pal_list = 'azure'),
                                     list(underlay = sdl_units |> 
                                            filter(SWSDLName %in%
                                                     unique(obj_sdl_to_plot$SWSDLName)),
                                         pal_list = 'gray')),
                setLimits = c(0,1)) +
  theme(axis.ticks = element_blank(), 
                         axis.text=element_blank())

# are there better ways to show this?

# a single sdl map for comparison
nf_maps_single <- obj_sdl_to_plot |>
    dplyr::filter(env_obj == 'NF1' & scenario == 'climatebaseadapt0') |>
  mutate(scenario = ifelse(scenario == 'MAX', 'MAX', 
                           paste0(climate_code, adapt_code)), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Proportion\nEWR achieved',
                plot_type = 'map',
                colorset = 'ewr_achieved',
                pal_list = achieve_pal,
                underlay_list = list(underlay = basin,
                                     pal_list = 'azure'),
                setLimits = c(0,1))  +
  theme(axis.ticks = element_blank(), 
                         axis.text=element_blank())

# a smaller subset of sdls and scenarios
# a single sdl map for comparison
obj_maps_few <- obj_sdl_to_plot |>
    dplyr::filter(env_obj %in% c('NF1', 'EF3', 'WB4') & 
                    scenario %in% c('climatedown2adapt250', 
                                    'climatebaseadapt0', 
                                    'climateup2adapt6500')) |>
  mutate(scenario = ifelse(scenario == 'MAX', 'MAX', 
                           paste0(climate_code, adapt_code)), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Proportion\nEWR achieved',
                plot_type = 'map',
                colorset = 'ewr_achieved',
                pal_list = achieve_pal,
                facet_row = 'scenario',
                                facet_col = 'env_obj',

                #underlay_list = list(underlay = basin,
                #                     pal_list = 'azure'),
                setLimits = c(0,1))  +
  theme(axis.ticks = element_blank(), 
                         axis.text=element_blank())

```

```{r}
#| label: fig-gauge-to-sdl-map
#| fig-cap: "Ecological objectives aggregated to SDL unit spatial scale, illustrated here for three examples (Priority ecosystem function 3, native fish 1, and waterbirds 4). Polygon colours indicate the proportion of EWR passed for each objective in each SDL unit. Aggregation as in @fig-spatial-scaling. Includes three climate scenarios: A (0.5x), E (historical base level/no change), and I (2x); and three adaptation options: 1 (no adaptation), 2 (+250 ML/d), and 3 (+6500 ML/d), though not all combinations [see Appendix @sec-map-versions]."

obj_maps_few

```

While we can see which scenarios are better or worse than others with bar graphs ([@fig-obj_in_groups]) or maps ([@fig-gauge-to-sdl-map]), they can make it difficult to accurately assess relative differences that may illuminate disproportionate impacts and thresholds for when adaption options could be most effectively adopted. In general, this requires quantitative x-axes which would typically be referenced to some baseline. Here, we can represent both our 'climate' and 'adaptation' scenarios on a single axis by quantifying the difference in mean flow from the baseline condition (historical hydrograph, E1) using the baselining functionality provided by HydroBOT (see @sec-baselining). Plotting the proportion of EWRs achieved for each of the native fish ecological objectives shows how that EWR achievement is related to changes in the overall levels of flow. One striking feature of @fig-difference-baseline is that the lines do not smoothly increase; EWR achievement is not simply a direct relationship to flow volumes. Instead, the timing matters. There are particularly steep slopes between the '1' (baseline) and '2' (+250 ML/d) adaptations. The application of water at those particular times is thus disproportionately impactful, yielding greater outcomes even when they have less water than the baseline adaptations ('1s') for climate scenarios 'A' and 'E'. Such analyses can identify highly efficacious points to add water to the system.

```{r}
#| label: fig-difference-baseline
#| fig-cap: "Quantitative scenario comparison for native fish objectives for three SDL units. Scenarios defined here as the difference in mean flow (GL) from the baseline historical scenario (E1). A key message here is that the lines are not smooth – EWR achievement is not simply a function of the amount of water. Instead, the 'adaptation' options, particularly the '2' scenarios of adding 250 ML/d of water, have disproportionately large impact (steep slopes). Shading indicates different ecological objectives within the native fish ecological value. This illustration includes three climate scenarios: A (0.5x), E (historical base level/no change), and I (2x) and three adaptation options: 1 (no adaptation), 2 (+250 ML/d), and 3 (+6500 ML/d)."
#| message: false


gl_difference <- obj_sdl_to_plot |>
  dplyr::filter(scenario %in% scenarios_to_plot) |>
  dplyr::filter(env_group == 'NF') |> 
  left_join(dif_flow, by = 'scenario') |> 
  # clean names
  mutate(SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |> 
  
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = "Proportion EWR achieved",
                x_col = "scenario_difference",
                x_lab = "Mean flow difference (GL)",
                colorgroups = 'env_group',
                colorset = 'env_obj',
                pal_list = env_pals,
                # base_lev = 'E1',
                # comp_fun = 'difference',
                # group_cols = c('env_obj', 'polyID'),
                color_lab = "Ecological objectives",
                facet_row = 'SWSDLName',
                facet_col = '.') 

gl_difference <- gl_difference + 
  geom_vline(data = filter(scenarios, scenario  %in% scenarios_to_plot),
             mapping = aes(xintercept = scenario_difference))

# This gets pretty close without needing the weird extra facet
# gl_difference <- gl_difference + 
#   geom_text(gl_difference$data |> filter(SWSDLName == "Lachlan"),
#             mapping = aes(label = scenario, y = Inf),
#             vjust = 1.5, size = 3)

header_difference <-  scenarios |> 
  filter(scenario  %in% scenarios_to_plot) |> 
  mutate(scenario = ifelse(scenario == 'MAX', 'MAX', 
                           paste0(climate_code, adapt_code))) |> 
  ggplot(aes(x = scenario_difference,
             y = 1)) +
  # geom_vline(mapping = aes(xintercept = scenario_difference)) +
  # ggrepel::geom_text_repel(mapping = aes(label = scenario),
  #                          # size = 3,
  #                          angle = 90,
  #                          direction='x') +
  # This kind of works better if we get rid of vline. The repel just isnt' working very well
  geom_text(mapping = aes(label = scenario), size = 3) +
  theme_werp_toolkit(legend.position = "none", axis.title=element_blank(),
                     axis.text=element_blank(), axis.ticks=element_blank())

# put those together
header_difference + gl_difference + 
  plot_layout(ncol = 1, heights = c(1,15)) + theme(legend.position = "right")
```

```{r}
#| label: build-smooth-figs
#| include: false
sdl_smooth_groups <- obj_sdl_to_plot |>
  dplyr::filter(adapt_code %in% c(1,2,3) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(env_group != 'EB') |> 
  # mutate(ewr_achieved = ewr_achieved + 1) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n'),
         adapt_code = as.character(adapt_code)) |>
  plot_outcomes(outcome_col = 'ewr_achieved',
                x_col = 'flow_multiplier',
                outcome_lab = 'Condition',
                x_lab = 'Flow relative to baseline',
                transx = 'log10',
                transy = 'log10',
                scales = 'free_y',
                
                color_lab = 'Adaptation',
                colorgroups = NULL,
                colorset = 'adapt_code',
                # point_group = 'env_obj',
                pal_list = adapt_pal,
                facet_row = 'env_group',
                facet_col = 'SWSDLName',
                position = 'jitter',                
                zero_adjust = 0.01, # needed here to make the trans work.

                base_list = list(base_lev = 'E1',
                                 comp_fun = 'relative',
                                 group_cols = c('env_obj', 'polyID')),
                smooth_arglist = list())

# This is better in some ways and worse in others, flippig the colors and facets
sdl_smooth_groups_flipped <- obj_sdl_to_plot |>
  dplyr::filter(adapt_code %in% c(1,2,3) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(env_group != 'EB') |> 
  # mutate(ewr_achieved = ewr_achieved + 1) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |>
  plot_outcomes(outcome_col = 'ewr_achieved',
                x_col = 'flow_multiplier',
                outcome_lab = 'Ecological objectives\nrelative to baseline',
                x_lab = 'Flow relative to baseline',
                transx = 'log10',
                transy = 'log10',
                scales = 'free_y',
                
                color_lab = 'Ecological values',
                colorgroups = NULL,
                colorset = 'env_group',
                # point_group = 'env_obj',
                pal_list = list('scico::berlin'),
                facet_row = 'adapt_code',
                facet_col = 'SWSDLName',
                position = 'jitter',
                zero_adjust = 0.01, # needed here to make the trans work.

                base_list = list(base_lev = 'E1',
                                 comp_fun = 'relative',
                                 group_cols = c('env_obj', 'polyID')),
                smooth_arglist = list())

# just native fish in the Namoi
# This is better in some ways and worse in others, flippig the colors and facets
sdl_smooth_groups_NFNamoi <- obj_sdl_to_plot |>
  dplyr::filter(adapt_code %in% c(1,2,3) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(env_group == 'NF' & SWSDLName == 'Namoi') |> 
  # mutate(ewr_achieved = ewr_achieved + 1) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n'),
         adapt_code = as.character(adapt_code)) |>
  plot_outcomes(outcome_col = 'ewr_achieved',
                x_col = 'flow_multiplier',
                outcome_lab = 'Condition',
                x_lab = 'Flow relative to baseline',
                transx = 'log10',
                transy = 'log10',
                scales = 'free_y',
                
                color_lab = 'Adaptation',
                colorgroups = NULL,
                colorset = 'adapt_code',
                # point_group = 'env_obj',
                pal_list = adapt_pal,
                facet_row = 'Target',
                facet_col = 'SWSDLName',
                position = 'jitter',                
                zero_adjust = 0.01, # needed here to make the trans work.

                base_list = list(base_lev = 'E1',
                                 comp_fun = 'relative',
                                 group_cols = c('env_obj', 'polyID')),
                smooth_arglist = list())


# What if we really try to smash down what we're showing? The adaptations are blowing it out. Which is a good message, but confusing. Maybe it's a two-parter?

# First, the climate response at no adaptation
sdl_smooth_clim <- obj_sdl_to_plot |>
  dplyr::filter(adapt_code %in% c(1) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(env_group != 'EB') |> 
  # mutate(ewr_achieved = ewr_achieved + 1) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |>
  plot_outcomes(outcome_col = 'ewr_achieved',
                x_col = 'flow_multiplier',
                outcome_lab = 'Condition',
                x_lab = 'Flow relative to baseline',

                transx = 'log10',
                transy = 'log10',
                scales = 'free_y',
                color_lab = 'Ecological values',
                colorgroups = NULL,
                colorset = 'Target',
                # point_group = 'env_obj',
                pal_list = Target_pal,
                facet_row = '.',
                facet_col = 'SWSDLName',
                position = 'jitter',
                zero_adjust = 0.01, # needed here to make the trans work.

                base_list = list(base_lev = 'E1',
                                 comp_fun = 'relative',
                                 group_cols = c('env_obj', 'polyID')),
                smooth_arglist = list())

# Next, the adaptation response at no climate
sdl_smooth_adapt <- obj_sdl_to_plot |>
  dplyr::filter(climate_code %in% c('E') &
                  scenario != 'MAX' & 
                  flow_addition < 10000 ) |> # max isn't really relevant here, and 12,500 is a LOT
  dplyr::filter(env_group != 'EB') |> 
  # get the log to work, saying we added 1 is fine here
  mutate(flow_addition = flow_addition + 1) |>
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n')) |>
  plot_outcomes(outcome_col = 'ewr_achieved',
                x_col = 'flow_addition',
                outcome_lab = 'Condition',
                x_lab = 'Flow addition (ML/d)',
                transx = 'log10',
                transy = 'log10',
                scales = 'free_y',
                color_lab = 'Ecological values',
                colorgroups = NULL,
                colorset = 'Target',
                # point_group = 'env_obj',
                pal_list = Target_pal,
                facet_row = '.',
                facet_col = 'SWSDLName',
                position = 'jitter',
                zero_adjust = 0.01, # needed here to make the trans work.
                base_list = list(base_lev = 'E1',
                                 comp_fun = 'relative',
                                 group_cols = c('env_obj', 'polyID')),
                smooth_arglist = list())
```

With a large number of scenarios, we can better characterise the way values respond along different axes of change. Here, we use smoothed fits to show trends in the performance of ecological objectives (and groups thereof) in response to multiplicative flow changes ('climate' scenarios) and pulsed flow additions ('adaptations') (@fig-smooth-climate-adapt, @fig-smooth-all). We use baselining (see @sec-baselining) to show the relative change on both the x (flow) and y (response) axes. Thus, any slope other than 1:1 represents a disproportionate shift in condition. For example, other species and native vegetation show steeper increases with increasing flow relative to baseline than decreasing flow, while native fish respond disproportionately to flow change across the range considered here. Moreover, because we use smoothed fits, we can identify thresholds or areas of flow change that are disproportionately more important (small changes in flow can yield large shifts in condition).

*RL: This is confusing without any discussion of what Fig 20 is. Do we need it here or can this para move to the supplement too?* *GH: Hmm. I seem to have made major changes to these figs but not the text. Fig 20 used to be here, and fig 13 used to be different. The idea was that panel C could be used to accentuate the main point of Fig 20, that the adaptations here have a way bigger impact than the climate. So, yes, we should move most of this to the appendix. If we keep C, we should keep a sentence about it. And maybe we should decide we don't need C at all and cut it back off the figure.* In @fig-smooth-all the y-axis scales are much larger than the x, and so proportionality is difficult to assess. However, this figure makes clear the nonlinear nature of the response to flow shift, particularly for the no adaptation scenario (1), as well as changes in sensitivity between adaptation options. The response to adapatation options (differences between the lines in @fig-smooth-all, slope of lines in @fig-smooth-climate-adapt panel b) tends to be much stronger than the response to climate (slopes of the lines in @fig-smooth-all and @fig-smooth-climate-adapt panel a), even though the shift in total water might be lower. Moreover, the adaptation options are not as sensitive to the climate scenarios; adaptation options other than '1' (no adaptation) show less change with climate in [@fig-smooth-all]. Thus, these 'adaptations' are increasing resilience to those holistic flow changes. Although these scenarios do not represent true adaptation options or climate scenarios, this shows that such changes to resilience are possible with targeted interventions, and HydroBOT provides the capacity to investigate them.

The responses seen in @fig-smooth-all and @fig-smooth-climate-adapt vary between SDL unit (columns) and ecological values, indicating different sensitivity to both 'climate' and 'adaptation' flow conditions across space and among ecological groupings. For example, although both the Lachlan and Macquarie-Castlereagh have no successful waterbird outcomes under baseline conditions, targeted flow conditions have a greater ability to increase waterbird success in the Lachlan.

```{r}
#| label: fig-smooth-climate-adapt
#| fig-cap: (a) Smoothed fits of shifts in condition (proportion of EWR achieved relative to the scenario with no climate or adaptation changes [E1]) of broad ecological values to relative changes in water availability (also relative to E1). All climate scenarios (without adaptation; adaptation = 1) are included. (b) The full combination of scenarios (all climate and adaptations options) are shown for the example of native fish in the Namoi. This illustrates the larger impact of the 'adaptation options' used here compared to the 'climate' shifts. Fitted lines are loess smooths. Separate lines are fit to each ecological value. Columns are SDL units. Y-axes are restricted to better visualise the majoity of the data; thus, a small amount of data is not plotted, but is accounted for in the loess fits. The full set of SDL units and groups as in c shown in Appendix @fig-smooth-all.
#| message: false
#| warning: false

# layout <- "
# AAAA
# BBBB
# ##CC
# "

# grp_smooths <- patchwork::wrap_plots(sdl_smooth_clim  + 
#                                        coord_cartesian(ylim = c(0.5, 2)),
#                       sdl_smooth_adapt) +
#   plot_annotation(tag_levels = 'a') +
#   plot_layout(guides = 'collect', nrow = 2)
# 
# namoi_plot <- guide_area() + sdl_smooth_groups_NFNamoi + plot_layout(guides = 'collect')
# 
# patchwork::wrap_plots(grp_smooths,
#                       namoi_plot) +
#   plot_layout(nrow = 2, heights = c(5,1)) +
#   plot_annotation(tag_levels = 'a')

# patchwork::wrap_plots(sdl_smooth_clim  + coord_cartesian(ylim = c(0.5, 2)),
#                       sdl_smooth_adapt,
#                       sdl_smooth_groups_NFNamoi,
#                       design = layout, byrow = FALSE) +
#   plot_annotation(tag_levels = 'a')
# 
# ((sdl_smooth_clim  + coord_cartesian(ylim = c(0.5, 2)) )/
#   (sdl_smooth_adapt) +
#   plot_layout(guides = 'collect')) +
#   (sdl_smooth_groups_NFNamoi + theme(legend.position = 'left')) +
#   plot_annotation( tag_levels = 'a')

#GD: sorry i was not A FAN OF THIS PLOT. it was TOO CONFUSING with different x axes and different data plotted. so have cut out the original panel b. 

sdl_smooth_clim + 
  coord_cartesian(ylim = c(0.5, 2))+
  labs(y = 'Condition relative\nto baseline')+
sdl_smooth_groups_NFNamoi+
  labs(y = 'Condition relative\nto baseline')+
  plot_layout(ncol = 1, heights = c(2,4), guides = "collect")+ 
  plot_annotation( tag_levels = 'a')

```

Scenarios will often be defined along more than one axis. In the demonstration here, we define both a flow multiplication axis as a proxy for 'climate' shifts, and a flow addition as a proxy for 'adaptations'. Moreover, it will often be the case that values will respond to several different aspects of the flow regime, for example the mean flow and the flow variance or the return interval of floods or low-flow periods. Heatmaps or contour surfaces provide a powerful tool for visualising these interacting responses ([@fig-contour], [@fig-heatmap]). By examining multiple driver axes, such plots can highlight important interactions and identify where thresholds occur and where change along one axis (e.g. adaptations) can mitigate change along the other (e.g. climate shifts). This ability to identify the axes that provide resilience or sensitivity to changes in others will be critically important for management uses of this toolkit targeting climate change adaptation. These assessments must also take spatial and value differences into account; the different ecological values and SDL units show very different patterns in how they respond to the interaction between the 'climate' and 'adaptation' scenarios ([@fig-contour]).

Perhaps most usefully, if scenarios are carefully developed to explore the range of potential change in the system, then the heatmap represents the response surface of values over this range. Developing the response surface can be iterative, with subsequent scenarios developed to increase resolution near areas of rapid change in outcome. This approach differs from the usual approach of assessing only a small set of scenarios targeting specific proposed actions or environmental conditions. Instead, by covering the range to generate a response surface, it can provide hypotheses about how proposed scenarios might perform and assess how sensitive the outcomes are to uncertainty in single scenarios. Specific proposed scenarios could of course be assessed as well and mapped onto the surface. For example, we might think of the flow multipliers here as defining the entire range of plausible climate shifts, and the addition scenarios as the entire potential range of additions. Then, specific proposed adaptations or climate sequences could be mapped onto these heatmaps at particular points. The reverse is also possible; identification of areas of high sensitivity in the heatmaps could be used to choose a set of potential adaptations designed to increase climate resilience. This idea extends to additional dimensions, which can then be collapsed in various ways (e.g. via principal component analysis, or by identifying dominant hydrometrics) to visualise with 2d heatmaps even when the response surface itself is best defined and studied in more dimensions.

```{r}
#| label: build-heatmaps
# Use the Target here to get to the big groups

# qualitative axes
qual_heatmap <- agged_data$Target |>
  dplyr::filter(adapt_code %in% c(1,2,3, 4) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(!is.na(Target)) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n'),
         Target = stringr::str_replace(Target, 'Priority e', 'E'),
         adapt_code = as.character(adapt_code)) |>
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Condition',
                y_col = 'adapt_code', 
                y_lab = 'Adaptation option',
                x_col = 'climate_code', 
                x_lab = 'Climate scenario',
                plot_type = 'heatmap',
                colorset = 'ewr_achieved',
                pal_list = 'grDevices::Viridis',
                facet_row = 'Target',
                facet_col = 'SWSDLName')

# Quantitative axes- it's more informative, but uglier
quant_heatmap <- agged_data$Target |>
  dplyr::filter(adapt_code %in% c(1,2,3) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(!is.na(Target)) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n'),
         Target = stringr::str_replace(Target, 'Priority e', 'E'),
         adapt_code = as.character(adapt_code)) |>
  mutate(flow_addition = flow_addition + 1) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Condition',
                y_col = 'flow_addition', 
                y_lab = 'Flow addition',
                x_col = 'flow_multiplier', 
                x_lab = "Flow multiplier",
                plot_type = 'heatmap',
                transy = 'log10',
                transx = 'log10',
                colorset = 'ewr_achieved',
                pal_list = 'grDevices::Viridis',
                facet_row = 'Target',
                facet_col = 'SWSDLName',
                setLimits = c(0,1))

# interpolated raster- just one argument different
  # THe spacing is too uneven though
quant_interp_heatmap <- agged_data$Target |>
  dplyr::filter(adapt_code %in% c(1,2,3) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(!is.na(Target)) |> 
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n'),
         Target = stringr::str_replace(Target, 'Priority e', 'E'),
         adapt_code = as.character(adapt_code)) |>
  mutate(flow_addition = flow_addition + 1) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = 'Condition',
                y_col = 'flow_addition', 
                y_lab = 'Flow addition',
                x_col = 'flow_multiplier', 
                x_lab = "Flow multiplier",
                plot_type = 'heatmap',
                transy = 'log10',
                transx = 'log10',
                colorset = 'ewr_achieved',
                pal_list = 'grDevices::Viridis',
                facet_row = 'Target',
                facet_col = 'SWSDLName',
                setLimits = c(0,1),
                contour_arglist = list(interpolate = TRUE))

# This would be better if we had a more even and tighter set of scenarios
  contour_heatmap <- agged_data$Target |>
  dplyr::filter(adapt_code %in% c(1,2,3) &
                  scenario != 'MAX') |> # max isn't really relevant here
  dplyr::filter(!is.na(Target)) |> 
  dplyr::mutate(Target = ifelse(Target == "Waterbird", "Waterbirds", Target))|>
  # clean names
  mutate(scenario = paste0(climate_code, adapt_code), 
         SWSDLName = stringr::str_replace(SWSDLName, '–', '-\n'),
         Target = stringr::str_replace(Target, 'Priority e', 'E'),
         adapt_code = as.character(adapt_code)) |>
  mutate(flow_addition = flow_addition + 1) |> 
  plot_outcomes(outcome_col = 'ewr_achieved',
                outcome_lab = "Proportion EWR achieved",
                y_col = 'flow_addition', 
                y_lab = 'Flow addition',
                x_col = 'flow_multiplier', 
                x_lab = "Flow multiplier",
                plot_type = 'heatmap',
                transy = 'log10',
                transx = 'log10',
                colorset = 'ewr_achieved',
                pal_list = achieve_pal, #'grDevices::Viridis', #keeping consistent with pallet you used above
                facet_row = 'Target',
                facet_col = 'SWSDLName',
                setLimits = c(0,1),
                contour_arglist = list()
  )

```

```{r}
#| label: fig-contour
#| fig-cap: Condition results visualised as a surface with the two scenario definitions on the axes. This approach allows visualising changes in outcome as the result of multiple axes on which scenarios might differ. These axes might be different aspects of scenario creation (as here), or they might be different axes describing the outcome of scenarios (e.g. two different hydrometrics such as mean flow and flow variance)

contour_heatmap
```

Causal networks can be used to show outcomes in addition to the causal relationships themselves. This sort of visualisation allows identification of the holistic consequences of the different scenarios across the complex and interrelated sets of values, as well as identifying nodes that are unusually resilient or sensitive to change ([@fig-network-subset]). For example, NF7 is little changed across the range of climate scenarios considered, while EF5 shows greater sensitivity. The network structure can also identify critical nodes that have outsize consequences for other nodes in the network. Such nodes are critically important for outcomes, and so should be investigated to determine whether they capture key aspects of the system or are instead an artifact of model structure (possibly resulting from the way the network was developed) that may bias results. If their sensitivity does in fact capture the system, these nodes may be ideal targets for management intervention. The edges also identify the dependence between values, where we can see that different environmental outcomes depend on not only different hydrologic indicators (EWRs), but also different numbers of those indicators.

```{r}
#| eval: false

# RERUN IF YOU NEED TO REMAKE CAUSAL NETWORK

# NETWORK 1) demonstrates steps in aggregation, some relative results, for one gauge

# # just get the theme-scale values
# # First we need the sequence lists
# 
aggparams <- yaml::read_yaml(file.path(agg_results_gauge,
                                      'agg_metadata.yml'))

themeseq <- aggparams$aggregation_sequence[
  !names(aggparams$aggregation_sequence) %in%
    c('planning_unit', 'sdl_units', 'mdb')
]

themefuns <- aggparams$aggregation_funsequence[
  !names(aggparams$aggregation_sequence) %in%
    c('planning_unit', 'sdl_units', 'mdb')
]

causal_ewr <- causal_ewr

# skip the _timing
ewr_edges <- make_edges(dflist = causal_ewr,
                        fromtos = themeseq[2:length(themeseq)],
                        gaugefilter = gauges_to_plot[2])

# get the nodes
nodes <- make_nodes(ewr_edges)

aggvals <- extract_vals_causal(agged_data,
                               whichaggs = themefuns,
                               valcol = 'ewr_achieved',
                               targetlevels = names(themeseq[2:length(themeseq)]))

# filter to a single gauge. Multiple gauges should have separate networks or otherwise split the gauge-specific nodes. And include the larger scales pertaining to that gauge.

# This takes a while (5 mins or so). Cutting to just the polygons and name-matchign gauges only saves about 30 seconds and introduces potential errors, so leaving it as-is.
system.time(gaugematch <-  st_intersects(bom_basin_gauges[bom_basin_gauges$gauge ==
                                                gauges_to_plot[2],],
                             aggvals, sparse = FALSE))

aggvals <- aggvals[as.vector(gaugematch),] |>
  st_drop_geometry()

# join to the nodes
nodes_with_vals <- nodes |>
  dplyr::filter(NodeType != 'target_5_year_2024') |>
  dplyr::left_join(aggvals) |>
  dplyr::filter(!is.na(scenario)) |>
  # Scenario metadata fell off, return it
  dplyr::left_join(scenarios, by = 'scenario') |>
  # clean up names
    dplyr::mutate(scenario = ifelse(scenario == 'MAX', 'MAX',
                           paste0(climate_code, adapt_code)),
         adapt_code = as.character(adapt_code))

# a subset of nodes so plots are readable
set.seed(17)
sampleenvobj <- nodes_with_vals |>
  filter(NodeType == 'env_obj') |>
  reframe(names = unique(Name)) |>
  pull() |>
  sample(10)

# pre-filter to make plotting simpler
basenodes <- nodes_with_vals |>
  dplyr::filter(flow_multiplier %in% c(0.5, 1, 2) & flow_addition == 0) |>
  baseline_compare(compare_col = 'scenario', base_lev = 'E1',
                   values_col = 'ewr_achieved',
                   group_cols = c('Name', 'NodeType', 'nodeorder'),
                   comp_fun = 'relative',
                   add_eps = 0.01) |>
  mutate(logrel_ewr_achieved = log(relative_ewr_achieved))
```

```{r}
#| eval: false

# RERUN IF YOU NEED TO REMAKE CAUSAL NETWORK

# #| label: make-networks
# #| include: false
# #| warning: false
# # use a subset again
aggNetworkdown_sub_rel <- basenodes |>
  dplyr::filter(flow_multiplier == 0.5 & flow_addition == 0) |>
  make_causal_plot(edges = ewr_edges,
                   focalnodes = sampleenvobj,
                 edge_pal = 'black',
                 node_pal = list(value = compare_pal),
                 node_colorset = 'logrel_ewr_achieved',
                 render = TRUE,
                 setLimits = c(min(basenodes$logrel_ewr_achieved),
                               0,
                               max(basenodes$logrel_ewr_achieved)),
                 save = TRUE,
                 savedir = file.path(demo_webdir,'images'),
                 savename = 'aggNetworkdown_sub_rel')

# GET LEGEND: THIS NEEDS TO BE INTEGRATED IN FUNCTION
lims <- c(min(basenodes$logrel_ewr_achieved),
                               0,
                               max(basenodes$logrel_ewr_achieved))

limtib <- tibble(x = seq(from = lims[1], to = lims[2], length.out = 10), y = 1) |>
  bind_rows(tibble(x = seq(from = lims[2], to = lims[3], length.out = 10), y = 1)) |>
  # make sure we have pretty options
  bind_rows(tibble(x = labeling::extended(lims[1], lims[3], 5), y = 1)) |>
  distinct()

limtib <- tibble(x = labeling::extended(lims[1], lims[3], 5), y = 1)

allcols <- limtib |>
  grouped_colors(pal_list = list(achieve_pal), colorset = 'x')

# Can I just make the legend and then steal it? using pretty breaks

pal_gg <- ggplot(allcols, aes(x = x, y = y, fill = color)) +
  geom_tile() +
  scale_fill_identity(guide = 'legend', breaks = allcols$color, labels = as.character(allcols$x))

# I guess I could also do a line and steal that, but if we've monkeyed with a dividing point, that won't work

limtc <- tibble(condition = seq(from = lims[1], to = lims[3], by = 0.1), y = 1)

pal_ggc <- ggplot(limtc, aes(x = condition, y = y, color = condition)) + geom_point() +
  paletteer::scale_color_paletteer_c(compare_pal)

net_legend <- ggpubr::get_legend(pal_ggc) |>
  ggpubr::as_ggplot()

ggsave(plot = net_legend, file.path(demo_webdir,'images', 'aggNetworkup_sub_legend.png'), width = 2, height = 4, units = 'cm')


```

# RADAR - Values across themes

radar plot?

```{r}
#| eval: false

# Not sure why this doens't run for me.
yMax = max(filter(obj_sdl_to_plot, scenario %in% scenarios_to_plot2 & !is.na(Target))$ewr_achieved)+0.05
ymin = 0 #RELATIVE VERSIONS OF THIS PLOT WILL HAVE VALUES LESS THAN ONE

#Attempt to make background
background_df <- data.frame(Order = as.factor(seq(1, length(unique(obj_sdl_to_plot$Target)))),
                            Target =  factor(unique(obj_sdl_to_plot$Target), levels = unique(obj_sdl_to_plot$Target)), 
                            y = max(obj_sdl_to_plot$ewr_achieved),
                            scenario = unique(obj_sdl_to_plot$scenario)[1])|>
    mutate(Order = as.numeric(Target))|>
  filter(!is.na(Target))


plt_2 <- obj_sdl_to_plot |>
  mutate(Order = as.numeric(Target))|>
  filter(scenario %in% scenarios_to_plot2 & !is.na(Target))|>
  ggplot(
      aes(
      x = reorder(stringr::str_wrap(Target, 6), Order),
      y = ewr_achieved,
      fill = climate_code)) +
  #Make background colours:
    geom_col(data = background_df,
           aes(x = reorder(stringr::str_wrap(Target, 6), Order), y = yMax, fill = Target), colour = NA, width = 1, alpha = 0.3)+
    geom_col(data = background_df,
           aes(x = reorder(stringr::str_wrap(Target, 6), Order), y = ymin, fill = Target), colour = NA, width = 1, alpha = 0.3)+
  # Make dashed lines:
  geom_hline(
    aes(yintercept = y),
    data.frame(y = c(ymin, 0, yMax/2, yMax)),
    color = "grey40", linetype = "longdash"
  ) +
  # Make scenario BARS - EWR achieved
   geom_col(position=position_dodge(),width=0.65,size=0.3) +
  # Make arrows:
  geom_segment(
    aes(
      x = stringr::str_wrap(Target, 6), y = ymin, 
      xend = stringr::str_wrap(Target, 6), yend = yMax),  
    linetype = "solid", color = "gray20",arrow = arrow(length = unit(0.25, "cm"), type = "closed"), linewidth = 0.2) + 
  coord_cartesian(ylim = c(ymin, yMax))+
  # Make it circular:
  coord_polar()+
  theme_werp_toolkit()+
  theme(text = element_text(size = 10), axis.title.x = element_blank(), axis.title.y = element_text(hjust=0.75))+
  scale_fill_manual(values = sceneTarget_pal, aesthetics = "fill", breaks = c("A", "E", "I"), name = "Climate scenario")+
  labs(y = "Proportion EWR achieved")

#plt_2


```

```{r}
#| eval: false

## Radar for graphical abstract
#FUDGEing data!!!
obj_sdl_to_plot_temp <- obj_sdl_to_plot |>
  filter(Target %in% c("Waterbirds", "Native fish", "Other species", "Native vegetation"))|>
  mutate(Target = as.character(Target))|>
  mutate(ewr_achieved = ifelse(Target == "Waterbirds", ewr_achieved + 0.3,
                               ifelse(Target == "Native fish", ewr_achieved/2+0.3,
                                      ifelse(Target == "Other species", flow_multiplier/4,
                                             ifelse(Target == "Native vegetation", flow_multiplier/4.5, NA)))),
         Target = case_when(Target == "Waterbirds" ~ "Agricultural benefits",
                            Target == "Native fish" ~ "Water allocation",
                            Target == "Other species" ~ "End of system flows",
                            Target == "Native vegetation" ~ "Social benefits"))
  
obj_sdl_to_plot_temp   <- bind_rows(obj_sdl_to_plot_temp, obj_sdl_to_plot)


obj_sdl_to_plot_temp   <- obj_sdl_to_plot_temp|>
  mutate(Target = as.character(Target))|>
  mutate(Target = factor(Target, levels = c("Native fish","Native vegetation","Waterbirds", "Other species", "Priority ecosystem function", "End of system flows","Water allocation","Agricultural benefits", "Social benefits")))|>
  arrange(Target)|>
  mutate(Order = as.numeric(Target))|>
  mutate(ewr_achieved = abs(ewr_achieved))

#Attempt to make background
background_df <- data.frame(Order = as.factor(seq(1, length(unique(obj_sdl_to_plot_temp$Target)))),
                            Target =  factor(unique(obj_sdl_to_plot_temp$Target), levels = unique(obj_sdl_to_plot_temp$Target)), 
                            y = max(obj_sdl_to_plot_temp$ewr_achieved),
                            scenario = unique(obj_sdl_to_plot_temp$scenario)[1])|>
    mutate(Order = as.numeric(Target))|>
  filter(!is.na(Target))


yMax = 0.65
ymin = 0 #RELATIVE VERSIONS OF THIS PLOT WILL HAVE VALUES LESS THAN ONE

plt_20 <- ggplot(filter(obj_sdl_to_plot_temp, scenario %in% c("climatebaseadapt0",  "climateup2adapt0") & !is.na(Target)),
      aes(
      x = reorder(stringr::str_wrap(Target, 6), Order),
      y = ewr_achieved,
      fill = climate_code)) +
  #Make background colours:
    geom_col(data = background_df,
           aes(x = reorder(stringr::str_wrap(Target, 6), Order), y = yMax, fill = Target), colour = NA, width = 1, alpha = 0.3)+
    geom_col(data = background_df,
           aes(x = reorder(stringr::str_wrap(Target, 6), Order), y = ymin, fill = Target), colour = NA, width = 1, alpha = 0.3)+
  # Make dashed lines:
  geom_hline(
    aes(yintercept = y),
    data.frame(y = c(ymin, 0, yMax/2, yMax)),
    color = "grey40", linetype = "longdash"
  ) +
  # Make scenario BARS - EWR achieved
   geom_col(position=position_dodge(),width=0.65,size=0.3) +
  # Make arrows:
  geom_segment(
    aes(
      x = stringr::str_wrap(Target, 6), y = ymin, 
      xend = stringr::str_wrap(Target, 6), yend = yMax),  
    linetype = "solid", color = "gray20",arrow = arrow(length = unit(0.25, "cm"), type = "closed"), linewidth = 0.2) + 
  coord_cartesian(ylim = c(ymin, yMax)) +
  # Make it circular:
  coord_polar() +
  theme_werp_toolkit()+
  theme(text = element_text(size = 10), axis.title.x = element_blank(), axis.title.y = element_text(hjust=0.75))+
  scale_fill_manual(values = sceneTarget2_pal, aesthetics = "fill", breaks = c("A", "E", "I"), name = "Climate scenario")+
  labs(y = "Proportion EWR achieved")

#plt_2

#ggsave(plot = plt_20, filename = file.path(demo_webdir,"images", paste0("radar_env", Sys.Date(), ".pdf")), device = "pdf", width = 5, height = 5)

```

```{r}
#| label: fig-radar
#| fig-cap: Condition results visualised as a radar plot with the three climate scenarios. This approach allows visualising changes in outcome of disparate thematic variables (as illustrated in the graphical abstract) and is useful for quick broad-scale representation of the results.

plt_2
```

# Implications and conclusions

HydroBOT provides consistent, scientifically robust, and repeatable capacity to model responses to flow, producing management-relevant outcomes at multiple dimensions. This capacity provides a step change for the Murray-Darling Basin Authority and other users to move beyond assessment of hydrologic outcomes and towards assessing the values that depend on that hydrology. Requirements such as those for a "healthy working basin" [@murray-darlingbasinauthority2011] that demand attention across socio-economic, cultural, and environmental values are commonly included in management plans for large basins [e.g. @deltastewardshipcouncil2013; @usdepartmentoftheinteriorbureauofreclamation2012; @ziolkowska2016; @connor2015], and reflect the interconnections of a range of values provided by these systems. Key to assessing this range of values is modeling their dependence on hydrology and other management levers, and aggregating the outcomes to management-relevant scales.

Large-scale natural resource management requires the capacity to make decisions relating to multiple spatial, temporal, and value dimensions, and is most successful when multiple scales within those dimensions are considered [@moore2021]. The HydroBOT aggregation and scaling framework navigates those dimensions for water-dependent social, economic, environmental, and cultural values, even when the underlying response models are highly detailed. Combining such disparate information in a standard and comparable manner can illuminate synergies and trade-offs, which could be critical for decision making. Moreover, the scaling provided by HydroBOT condenses and synthesizes large, complex outcomes to results tailored for interpretation for management questions. Our example concerns the ecological values of the Murray-Darling Basin; however, the framework is equally applicable to social, economic, and cultural values and the toolkit itself has a modular design to incorporate disparate response models.

Water management faces an increasing need and expectation for scientifically-sound and interpretable modelling from a range of stakeholders. These issues are magnified when facing major long-range decision making to understand the impacts of climate change and potential policy or operational adaptations management agencies might undertake. Co-design of HydroBOT with a diverse team within the Murray-Darling Basin Authority yielded distinct benefits to achieving these goals, with developers and researchers better understanding and building in the needs of the MDBA, while the MDBA better understands the limitations, uses, and potential of the tool. The focus on communication both within MDBA and to external partners, including model provenance, production of tailored synthesis figures, and causal networks, is a key outcome to build trust and understanding in HydroBOT as a tool and the results it produces. HydroBOT provides new capacity to assess a wide range of target values across scenarios and exciting opportunity for continued advances and improvement with the development of new response models and climate scenarios.

### Outline

What we achieved:

-   a toolkit that provides a consistent, scientifically robust and repeatable capacity to model responses to flow, with management-relevant outputs

-   co-design and development between scientists and managers

What this provides:

-   new ability to assess the wide suite of values the MDBA is mandated to manage for.

    -   moving beyond hydrology

-   Clear (or at least open) modelling to build trust

-   ability to assess a range of scenarios, including climate and climate adaptation

Advantages:

-   Identification of areas of resilience and sensitivity.

-   Uncertainty, understanding parameter space.

-   Modularity

-   adapt best models for responses into unified model and target management relevance

-   agnostic to scenario

-   causal networks

What cool software things do we do?

-   Modularity in architecture

-   ability to wrap models in multiple languages

-   flexibility- e.g. ability to aggregate differently

-   consistent comparison, with data provenance

-   metadata and use as parameters for repeatability

### From elsewhere- incorporate

*lots of this can be the backbone of the big-picture points above*

We expand upon the utility of an existing diver-indicator model by linking its indicators *EF: indicators same as values here I suppose? I think cos this paper is quite dense and tackles a lot of different types of variables and data visualisation techniques, it's really important to use consistent terms.* to objectives. This increases the transparency in the causal relationships that underpin the model and builds understanding and trust in the outcomes.

concerning which adaptation options should be implemented under changing climate by explicitly modelling effects on both ecological and other values.

The development of HydroBOT is necessary to better understand the impacts of climate change and the different adaptation options in response to climate change, on water-dependent social, economic, environmental and cultural values. It incorporates new and existing information, knowledge and models to enable transparent, repeatable assessments of impacts and adaptation to future climates. In summary, the toolkit ingests scenarios (for example, climate or flow timeseries), feeds them to a response model (such as the EWR tool), reports outcomes and enables comparisons among scenarios. It uses the links in the response models for visualisation of the complex inter-relationships between water-dependent outcomes. This aids transparency and improves communication of the outputs.

# CRediT authorship contribution statement
Galen Holt: Writing – original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Georgia Dwyer: Writing – original draft, Visualization, Methodology,Investigation, Formal analysis, Conceptualization. David Robertson: Writing – review & editing, Supervision, Conceptualization, Funding acquisition. Martin Job: Writing – review & editing, Supervision, Conceptualization. Lara Palmer: Writing – review & editing, Supervision, Conceptualization. Rebecca E Lester Writing – review & editing, Supervision, Investigation, Conceptualization, Funding acquisition.

# Declaration of competing interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

# Acknowledgements
The work is supported by the Murray–Darling Water and Environment Research Program. *Rebecca Check*

# Data availability
All data and their sources have been explained in the manuscript.

# References

::: {#refs}
:::

# Appendix 1 {#sec-appendix1}

## Glossary {#sec-glossary}

```{r}
#| label: sec-glossary
#| tbl-cap: Glossary adapted from the Murray-Darling Basin Long Term Watering Plans (LTWPs) (@nswdepartmentofplanningandenvironment2020). 

gloss_tab <- readr::read_csv(file.path(demo_webdir, "definitions.csv"))|>
  dplyr::filter(!is.na(Definition))

knitr::kable(gloss_tab)

```


## Component overview

The HydroBOT architecture comprises three major components, the Controller, Aggregator and Comparer, that receive data and information from several input sources, including input scenarios, hydrological modelling, spatial data, causal networks and response models ([@fig-architecture, @tbl-components]). Here we detail some specific details about each of these components and other input sources.

![Architecture of toolkit. Hydrographs representing are inputs, typically reflecting modeled scenarios or historic flows. The flow of data during a modeling run follows the bold arrows, with the additional components of the Causal network and Spatial units providing necessary grouping information for the Aggregator. Dashed lines indicate that hydrographs and direct Response model output can utilise the Comparer functionality directly. Causal networks are linked closely to each Response model, and while both are defined externally to the toolkit they require significant work to integrate into a compatible, modular toolkit component. Spatial units are typically more general, typically polygons of management interest, and require only light changes to make compatible. Each set of boxes represents a distinct component of the toolkit, allowing changes to be made at any step in a modular way without impacting the functioning of other stages.](../images/architecture.png){#fig-architecture}

```{r}
#| label: tbl-components
#| tbl-cap: Components of HydroBOT architecture

comp_tab <- readr::read_csv('presentation_paper/component_table.csv', show_col_types = FALSE)

knitr::kable(comp_tab)
# huxtable::huxtable(comp_tab) |> theme_article()
```

### Aggregation

Aggregation functions determine outcomes and reflect processes or values. Aggregating a set of outcomes yields a different outcome depending on the function used. Thus, function choice should be considered carefully and reflect the processes involved or management goals. HydroBOT provides default functions for the mean, compensating (e.g. max), limiting (min), and spatially-weighted versions of the same. It also provides the ability for the user to define any desired function, allowing for more complex situations.

#### Spatial aggregation

Maps allow large-scale visualization of ecological objectives under various scenarios. These visualisations can be quite important for communications and quickly grasping how outcomes aggregate spatially ([@fig-spatial-scaling]) and spatial patterns in the data ([@fig-gauge-to-sdl-map]). Management targets are often defined spatially, and in the case of EWRs, they are defined in local Planning Units (which are constituents of the SDL units defined above), where outcomes may depend on hydrographs at one or more gauges. Representing the outcomes as maps can provide intuitive assessment of the condition of values across space and whether different spatial scales or spatial locations are responding differently to scenarios. Moreover, the ecology (in this example) or other processes might themselves be large-scale, and so capturing the condition over a large area is a better descriptor of the true outcome than assessing each specific location separately. For example, objective EF3 is "Provide movement and dispersal opportunities for water dependent biota to complete lifecycles and disperse into new habitats within catchments", and so necessarily incorporates a spatial dimension. Particularly in these situations, the aggregation method should be considered carefully -- for movement opportunities to succeed, perhaps the success of the SDL unit should be determined by the lowest value at a gauge if it represents a loss of connectivity. In contrast, for WB4: Increase opportunities for colonial waterbird breeding, it might be sufficient if a single site within the SDL unit provides those opportunities.

```{r}
#| label: fig-spatial-scaling
#| fig-cap: Environmental water requirements (EWRs) are defined by hydrographs at gauges (a), and apply to Planning Units (b). The outcomes at these scales of definition can then be aggregated to larger spatial areas, such as SDL units (c), which we do here with an area-weighted mean for the example EWR NF1 in the baseline climate and adaptation scenario (E1, simple historical hydrograph with no climate or adaptation changes).


nf_gauges + nf_PU + nf_maps_single +
  plot_layout(guides = 'collect') &
  theme(legend.position = 'bottom')
```

#### Temporal aggregation

#### Objective aggregations

## Response models {#sec-ewr-table}

#### EWRs

```{r}
#| eval: false
#| label: tbl-ewrs
#| tbl-cap: !expr glue::glue("Environmental water requirements (EWR) for two example gauges ({gauges_to_plot[1]} and {gauges_to_plot[2]}).")


ewrs_in_pyewr |> 
  filter(Gauge %in% gauges_to_plot) |> 
  # Why these? Same as Georgia had
  dplyr::select(Catchment = LTWPShortName, Gauge, Code, StartMonth, EndMonth, TargetFrequency, TargetFrequencyMax, TargetFrequencyMin, EventsPerYear, Duration, MinSpell, FlowThresholdMin, FlowThresholdMax, `MaxInter-event`) |>  
  # huxtable::huxtable() |>
  # theme_article()
  knitr::kable()
```

# Appendix 2

## Scenarios {#sec-scenarios}

```{r}
#| label: tbl-scenarios
#| tbl-cap: Demonstration scenarios are a factorial combination of 'climate' (scaled flow) and 'adaptation' (pulsed additions). Climate scenarios included in this demonstration were produced by applying a flow multiplier to historical flows. Adaptation options were applied to each climate scenario with additional flows added throughout the period of September to December. For ease of display in some figures, we provide alpha codes for the 'climate' changes and numeric codes for the 'adaptations'. Mostly frequently, we present the ‘A’ (0.5 x historical flow), ‘E’ (1.0 x historical flow and ‘I’ (2.0 x historical flow) scenarios, with adaptions 1 (no flow addition), 2 (addition of 250 ML/d) and 3 (addition of 6500 ML/d).
#| layout-ncol: 2
#| tbl-subcap: 
#|  - "Climate"
#|  - "Adaptation"


adapt_scenes <- scenarios |> 
  filter(scenario != 'MAX') |> 
  mutate(flow_addition = as.integer(flow_addition)) |> 
  select(`Adaptation code` = adapt_code,
         `Flow addition (ML/d)` = flow_addition) |>
  distinct()
climate_scenes <- scenarios |> 
  filter(scenario != 'MAX') |> 
  select(`Climate code` = climate_code,
         `Flow multiplier` = flow_multiplier) |>
  distinct()

# Huxtables look better, but won't go side-by-side. and despite saying they work better in word, they're much worse.

# huxtable::huxtable(climate_scenes) |> theme_article()
# huxtable::huxtable(adapt_scenes) |> theme_article()

knitr::kable(climate_scenes |> 
               mutate(`Flow multiplier` = signif(`Flow multiplier`, 2)))
knitr::kable(adapt_scenes)

```

```{r}
#| label: fig-hydrographs
#| fig-cap: Hydrographs for two example gauges (represented by colour) with 'climate' scenarios on rows and 'adaptation' scenarios as columns. Scenario codes as in @tbl-scenarios.

hydro_plot <- scenehydros |>
    dplyr::filter(scenario %in% scenarios_to_plot & gauge %in% gauges_to_plot) |>
  dplyr::mutate(flow = flow/1000) |> 
  plot_outcomes(outcome_col = 'flow',
                outcome_lab = 'Flow (GL/day)',
                x_col = 'Date',
                colorset = 'gauge',
                color_lab = 'Gauge ID:',
                pal_list = gauge_pal,
                facet_row = 'climate_code',
                facet_col = 'adapt_code',
                ) +
    ggplot2::scale_y_continuous(sec.axis = sec_axis(~ . ,
                                                    name = "Climate scenario",
                                                    breaks = NULL, labels = NULL)) +
    ggplot2::scale_x_date(sec.axis = sec_axis(~ . , 
                                              name = "Adaptation option", 
                                              breaks = NULL, labels = NULL)) +
    theme_werp_toolkit(legend.position = "bottom")

hydro_plot
```

## Baselined hydrographs {#sec-baselining}

```{r}
#| label: fig-baseline-hydro-clim
#| fig-cap: Change in flow relative to the baseline scenario. These are flat lines because the relativisation occurs at each timepoint.The baseline scenario is represented by the observed historical data with no climate change or adaptaion options applied (scenario E1)


# Relative is how we should look at this for climate scenarios, but when the addition in the adaptation options happens when the baseline is at 0 or close to it, it goes to inf. So make two plots, I think.

# The relative one is supremely uninteresting, but maybe we need it to make a point?
base_hydro_clim <- scenehydros |>
    dplyr::filter(scenario %in% scenarios_to_plot & 
                    gauge %in% gauges_to_plot & 
                    flow_addition == 0) |>
    plot_outcomes(outcome_col = 'flow',
                  outcome_lab = 'Relative flow',
                  x_col = 'Date',
                  colorset = 'gauge',
                  color_lab = 'Gauge ID:',
                  pal_list = gauge_pal,
                  base_list = list(base_lev = "climatebaseadapt0",
                     values_col = 'flow',
                     comp_fun = "relative",
                     group_cols = c('Date', 'gauge')),
                  facet_col = 'climate_code')

 base_hydro_clim <- base_hydro_clim +
   ggplot2::scale_x_date(sec.axis = sec_axis(~ . ,
                                              name = "Climate scenarios",
                                              breaks = NULL, labels = NULL)) +
    theme_werp_toolkit(legend.position = "bottom")+
      guides(colour=guide_legend(nrow=2,byrow=TRUE))+
   ylab("Relative flow to baseline")

base_hydro_clim
```

```{r}
#| label: fig-baseline-hydro-adapt
#| fig-cap: Change in flow volume compared to the baseline scenario. This comparison is done using the difference, and so represents the flow additions. The baseline scenario is represented by the observed historical data with no climate change or adaptaion options applied (scenario E1)
#| 
# Look at difference just at the the base multiplier
base_hydro_adapt <- scenehydros |>
    dplyr::filter(scenario %in% scenarios_to_plot & 
                    gauge %in% gauges_to_plot & 
                    flow_multiplier == 1) |>
      plot_outcomes(outcome_col = 'flow',
                  outcome_lab = 'Difference flow',
                  x_col = 'Date',
                  colorset = 'gauge',
                  color_lab = 'Gauge ID:',
                  pal_list = gauge_pal,
                  base_list = list(base_lev = "climatebaseadapt0",
                     values_col = 'flow',
                     comp_fun = "difference",
                     group_cols = c('Date', 'gauge')),
                  facet_col = 'adapt_code') +
    ggplot2::scale_x_date(sec.axis = sec_axis(~ . ,
                                              name = "Adaptation options",
                                              breaks = NULL, labels = NULL)) +
    theme_werp_toolkit(legend.position = "bottom")+
      guides(colour=guide_legend(nrow=2,byrow=TRUE))+
   ylab("Flow difference from baseline")

base_hydro_adapt
```

# Appendix 3 {#sec-causalnetwork-versions}

## Additional causal network plots

#### 

# Appendix 4

## Additional comparison plots

### Map aggregation {#sec-map-versions}

The goal here is to show the values at the sdl scale, but also to show how the gauges aggregate to that scale. In the text, @fig-gauge-to-sdl-map has a few examples of SDL-scaled outcomes, while @fig-spatial-scaling has the scaling for one. @fig-gauge-to-sdl-map-all shows the SDL outcomes for all ecological objectives contributing the priority ecosystem function, along with the constituent gauges.

```{r}
#| label: fig-gauge-to-sdl-map-all
#| fig-cap: "all panels version." 

# reduced set of objectives and scenarios- closer to the next option
(ef_maps + theme(legend.position = 'none') ) + 
  wrap_elements(nf_gauges + 
                  guides(color = guide_colourbar(title.position="top")) +
                  theme(legend.position = 'bottom')) + 
  plot_annotation( tag_levels = 'a') +
  plot_layout(widths = c(2,1))


```

The full set of SDL units and ecological values

```{r}
#| label: fig-smooth-all
#| fig-cap: Smoothed fits to assess change in performance across the 'climate' scenarios. Points are individual ecological objectives, fitted lines are loess smooths. Separate fits are done for each adaptation option, and so differences between lines of different colors represents the impact of those adaptations. Rows are ecological values groupings, columns are SDL units. Note the very different scale of the y-axis. 
#| message: false  


# sdl_smooth_groups / sdl_smooth_clim +
#   plot_layout(guides = 'collect', heights = c(3,1))
sdl_smooth_groups # + coord_cartesian(ylim = c(0.5, 2)) # + geom_abline(slope = 1, intercept = 1)
```

### Heatmaps

```{r}
#| label: fig-heatmap
#| fig-cap: Condition results visualised as a heatmap with the two scenario definitions on the axes. This approach allows visualising changes in outcome as the result of multiple axes on which scenarios might differ. These axes might be different aspects of scenario creation (as here), or they might be different axes describing the outcome of scenarios (e.g. two different hydrometrics such as mean flow and flow variance)

qual_heatmap
```

### Causal networks

The causal networks shown in the text have been subset for readability, and baselined. Here, we have the ones they're based on. *I've turned these off for word because diagrammer hates word and I don't have the energy to do a workaround*.

```{r}
#| label: fig-full-networks
#| fig-cap: !expr glue::glue("Subset of the causal network for gauge {gauges_to_plot[2]} for all ecological objectives. The data is at the gauge scale for the first and second columns (EWRs and ecological objectives) and at the SDL unit scale for the third (ecological values). Thus, the final set of nodes contain information from other gauges that are not shown here for clarity. Colors represent the condition for the halving (a), baseline (b), and doubling (c) 'climate' scenario. Low condition is dark blue, high is light yellow.")
#| 
# aggNetworkdown <- nodes_with_vals |> 
#   dplyr::filter(flow_multiplier == 0.5 & flow_addition == 0) |> 
#   make_causal_plot(edges = ewr_edges,
#                  edge_pal = 'black',
#                  node_pal = list(value = achieve_pal),
#                  node_colorset = 'ewr_achieved',
#                  setLimits = c(0,1),
#                  render = FALSE,
#                  save = TRUE,
#                  savedir = 'images',
#                  savename = 'aggNetworkdown')
# 
# aggNetworkbase <- nodes_with_vals |> 
#   dplyr::filter(flow_multiplier == 1 & flow_addition == 0) |> 
#   make_causal_plot(edges = ewr_edges,
#                  edge_pal = 'black',
#                  node_pal = list(value = achieve_pal),
#                  node_colorset = 'ewr_achieved',
#                  setLimits = c(0,1),
#                  render = FALSE,
#                  save = TRUE,
#                  savedir = 'images',
#                  savename = 'aggNetworkbase')
# 
# aggNetworkup <- nodes_with_vals |> 
#   dplyr::filter(flow_multiplier == 2 & flow_addition == 0) |> 
#   make_causal_plot(edges = ewr_edges,
#                  edge_pal = 'black',
#                  node_pal = list(value = achieve_pal),
#                  node_colorset = 'ewr_achieved',
#                  setLimits = c(0,1),
#                  render = FALSE,
#                  save = TRUE,
#                  savedir = 'images',
#                  savename = 'aggNetworkup')

# DiagrammeR::render_graph(aggNetworkdown)
# DiagrammeR::render_graph(aggNetworkbase)
# DiagrammeR::render_graph(aggNetworkup)

# make the legend
# lim_full <- tibble(condition = seq(from = 0, to = 1, by = 0.1), y = 1)
# 
# pal_full <- ggplot(lim_full, aes(x = condition, y = y, color = condition)) + geom_point() +
#   paletteer::scale_color_paletteer_c(achieve_pal)
# 
# fullnet_legend <- ggpubr::get_legend(pal_full) |> 
#   ggpubr::as_ggplot()

aggNetworkdown_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkdown.png')) |>
  grid::rasterGrob(interpolate = TRUE)
aggNetworkbase_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkbase.png')) |>
  grid::rasterGrob(interpolate = TRUE)
aggNetworkup_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkup.png')) |>
  grid::rasterGrob(interpolate = TRUE)

layout <- "
AAAAABBBBB
CCCCC##D##
"
patchwork::wrap_plots(aggNetworkdown_grob, 
                      aggNetworkbase_grob, 
                      aggNetworkup_grob,
                      #fullnet_legend,
                      design = layout, byrow = FALSE) +
  plot_annotation(tag_levels = list(c('a', 'b', 'c')))

```

The same set of nodes as in the text, but the actual condition values, rather than relativized.

```{r}
#| label: fig-network-notbaseline
#| fig-cap: "Causal networks as in @fig-network-subset, but with the raw condition values for the halved (a), baseline (b), and doubled (c) scenarios instead of relativized to the baseline."

# aggNetworkdown_sub <- nodes_with_vals |> 
#   dplyr::filter(flow_multiplier == 0.5 & flow_addition == 0) |> 
#   make_causal_plot(edges = ewr_edges,
#                    focalnodes = sampleenvobj,
#                  edge_pal = 'black',
#                  node_pal = list(value = achieve_pal),
#                  node_colorset = 'ewr_achieved',
#                  setLimits = c(0,1),
#                  render = FALSE,
#                  save = TRUE,
#                  savedir = 'images',
#                  savename = 'aggNetworkdown_sub')
# 
# aggNetworkbase_sub <- nodes_with_vals |> 
#   dplyr::filter(flow_multiplier == 1 & flow_addition == 0) |> 
#   make_causal_plot(edges = ewr_edges,
#                    focalnodes = sampleenvobj,
#                  edge_pal = 'black',
#                  node_pal = list(value = achieve_pal),
#                  node_colorset = 'ewr_achieved',
#                  setLimits = c(0,1),
#                  render = FALSE,
#                  save = TRUE,
#                  savedir = 'images',
#                  savename = 'aggNetworkbase_sub')
# 
# aggNetworkup_sub <- nodes_with_vals |> 
#   dplyr::filter(flow_multiplier == 2 & flow_addition == 0) |> 
#   make_causal_plot(edges = ewr_edges,
#                    focalnodes = sampleenvobj,
#                  edge_pal = 'black',
#                  node_pal = list(value = achieve_pal),
#                  node_colorset = 'ewr_achieved',
#                  setLimits = c(0,1),
#                  render = FALSE,
#                  save = TRUE,
#                  savedir = 'images',
#                  savename = 'aggNetworkup_sub')
# 
# # DiagrammeR::render_graph(aggNetworkdown_sub)
# # DiagrammeR::render_graph(aggNetworkbase_sub)
# # DiagrammeR::render_graph(aggNetworkup_sub)

aggNetworkdown_sub_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkdown_sub.png')) |>
  grid::rasterGrob(interpolate = TRUE)
aggNetworkbase_sub_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkbase_sub.png')) |>
  grid::rasterGrob(interpolate = TRUE)
aggNetworkup_sub_grob <- png::readPNG(file.path(demo_webdir, 'images/aggNetworkup_sub.png')) |>
  grid::rasterGrob(interpolate = TRUE)

layout2 <- "
AAAAAAAAAAA#
BBBBBBBBBBBD
CCCCCCCCCCC#
"
patchwork::wrap_plots(aggNetworkdown_sub_grob, 
                      aggNetworkbase_sub_grob, 
                      aggNetworkup_sub_grob,
                      #fullnet_legend,
                      design = layout2) +
  plot_annotation(tag_levels = list(c('a', 'b', 'c')))
```

------------------------------------------------------------------------

# Journal info

## Environmental Modelling & Software

• Author names and affiliations. Where the family name may be ambiguous (e.g., a double name), please indicate this clearly. Present the authors' affiliation addresses (where the actual work was done) below the names. Indicate all affiliations with a lower-case superscript letter immediately after the author's name and in front of the appropriate address. Provide the full postal address of each affiliation, including the country name and, if available, the e-mail address of each author.

• Corresponding author. Clearly indicate who will handle correspondence at all stages of refereeing and publication, also post-publication. Ensure that telephone numbers (with country and area code) are provided in addition to the e-mail address and the complete postal address. Contact details must be kept up to date by the corresponding author.

• Present/permanent address. If an author has moved since the work described in the article was done, or was visiting at the time, a 'Present address' (or 'Permanent address') may be indicated as a footnote to that author's name. The address at which the author actually did the work must be retained as the main, affiliation address. Superscript Arabic numerals are used for such footnotes.

### Authorship

Authorship should be limited to those who have made a significant contribution to the conception, design, execution, or interpretation of the reported study. All those who have made significant contributions should be listed as co-authors. Where there are others who have participated in certain substantive aspects of the research project, they should be acknowledged or listed as contributors. Acknowledgement of the contributions of authors is encouraged (see Acknowledgements section below). The corresponding author should ensure that all appropriate co-authors and no inappropriate co-authors are included on the paper, and that all co-authors have seen and approved the final version of the paper and have agreed to its submission for publication.

Title, Authors, Affiliations and Contact details

### Abbreviations

Define abbreviations that are not standard in this field in a footnote to be placed on the first page of the article. Such abbreviations that are unavoidable in the abstract must be defined at their first mention there, as well as in the footnote. Ensure consistency of abbreviations throughout the article.

## Separate files

### Highlights

Highlights are optional yet highly encouraged for this journal, as they increase the discoverability of your article via search engines. They consist of a short collection of bullet points that capture the novel results of your research as well as new methods that were used during the study (if any). Please have a look at the examples here: example Highlights.

Highlights should be submitted in a separate editable file in the online submission system. Please use 'Highlights' in the file name and include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point).

Highlights are mandatory for this journal. They consist of a short collection of bullet points that convey the core findings of the article and should be submitted in a separate file in the online submission system. Please use 'Highlights' in the file name and include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point). See https://www.elsevier.com/highlights for examples.

### Graphical abstract

A Graphical abstract is optional and should summarize the contents of the article in a concise, pictorial form designed to capture the attention of a wide readership online. Authors must provide images that clearly represent the work described in the article. Graphical abstracts should be submitted as a separate file in the online submission system. Image size: Please provide an image with a minimum of 531 × 1328 pixels (h × w) or proportionally more. The image should be readable at a size of 5 × 13 cm using a regular screen resolution of 96 dpi. Preferred file types: TIFF, EPS, PDF or MS Office files. See https://www.elsevier.com/graphicalabstracts for examples.

### Abstract (not included in section numbering)

A concise and factual abstract is required, with a restriction of 150 words. The abstract should state briefly the purpose of the research, the principal results and major conclusions. An abstract is often presented separately from the article, so it must be able to stand alone. For this reason, References should be avoided, but if essential, then cite the author(s) and year(s). Also, non-standard or uncommon abbreviations should be avoided, but if essential they must be defined at their first mention in the abstract itself.

### Keywords

Immediately after the abstract, provide a maximum of 6 keywords, using American spelling and avoiding general and plural terms and multiple concepts (avoid, for example, 'and', 'of'). Be sparing with abbreviations: only abbreviations firmly established in the field may be eligible. These keywords will be used for indexing purposes.

### Software and/or data availability

Most EMS papers should include a software/data availability section containing as much of the following information as possible: name of software or dataset, developer and contact information, year first available, hardware required, software required, availability and cost. Also for software: program language, program size; for data: form of repository (database, files, spreadsheet), size of archive, access form. Note that "Contact the author" is not acceptable for software or data access. Please use online data and software storage and retrieval systems such as GitHub, BitBucket, FigShare, HydroShare or others to make your data and software readily available. Links to commercial software and data access web sites are also acceptable.

When a software component is an essential part of the paper, authors should be prepared to make it available to reviewers during the review process. To preserve the anonymity of reviewers, the authors should make the software available for a download, protecting it if needed by a password that is communicated to the editors.
